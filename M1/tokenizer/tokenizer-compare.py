"""
Skrypt por√≥wnawczy tokenizer√≥w z wizualizacjƒÖ w terminalu.
Por√≥wnuje efektywno≈õƒá tokenizacji r√≥≈ºnych tekst√≥w przez r√≥≈ºne tokenizery.
"""

from pathlib import Path
from tokenizers import Tokenizer
from transformers import AutoTokenizer
from rich.console import Console
from rich.panel import Panel
from typing import Optional

console = Console()

# ≈öcie≈ºki do tokenizer√≥w
TOKENIZERS_DIR = Path("tokenizers")

# Tokenizery BPE (Bielik + w≈Çasne)
BPE_TOKENIZERS = {
    "bielik-v1": TOKENIZERS_DIR / "bielik-v1-tokenizer.json",
    "bielik-v2": TOKENIZERS_DIR / "bielik-v2-tokenizer.json",
    "bielik-v3": TOKENIZERS_DIR / "bielik-v3-tokenizer.json",
    "pan-tadeusz": TOKENIZERS_DIR / "tokenizer-pan-tadeusz.json",
    "wolnelektury": TOKENIZERS_DIR / "tokenizer-wolnelektury.json",
    "nkjp": TOKENIZERS_DIR / "tokenizer-nkjp.json",
    "all-corpora": TOKENIZERS_DIR / "tokenizer-all-corpora.json",
}

# Tokenizer Herbert (WordPiece) - wymaga innego ≈Çadowania
HERBERT_DIR = TOKENIZERS_DIR / "herbert"

# Teksty testowe
TEST_TEXTS = {
    "Pan Tadeusz Ksiƒôga 1": Path("../korpus-wolnelektury/pan-tadeusz-ksiega-1.txt"),
    "Pickwick Papers": Path("../korpus-mini/the-pickwick-papers-gutenberg.txt"),
    "Fryderyk Chopin": Path("../korpus-mini/fryderyk-chopin-wikipedia.txt"),
}


def load_bpe_tokenizer(path: Path) -> Optional[Tokenizer]:
    """≈Åaduje tokenizer BPE z pliku JSON."""
    if not path.exists():
        return None
    try:
        return Tokenizer.from_file(str(path))
    except Exception as e:
        console.print(f"[red]B≈ÇƒÖd ≈Çadowania {path}: {e}[/red]")
        return None


def load_herbert_tokenizer(path: Path):
    """≈Åaduje tokenizer Herbert (WordPiece) z folderu."""
    if not path.exists():
        return None
    try:
        return AutoTokenizer.from_pretrained(str(path))
    except Exception as e:
        console.print(f"[red]B≈ÇƒÖd ≈Çadowania Herbert: {e}[/red]")
        return None


def count_tokens_bpe(tokenizer: Tokenizer, text: str) -> int:
    """Zlicza tokeny dla tokenizera BPE."""
    encoded = tokenizer.encode(text)
    return len(encoded.ids)


def count_tokens_herbert(tokenizer, text: str) -> int:
    """Zlicza tokeny dla tokenizera Herbert."""
    encoded = tokenizer.encode(text)
    return len(encoded)


def load_text(path: Path) -> str:
    """Wczytuje tekst z pliku."""
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()


def visualize_results(text_name: str, results: dict[str, int]):
    """
    Wizualizuje wyniki tokenizacji jako poziome s≈Çupki w terminalu.
    Sortuje od najmniejszej do najwiƒôkszej liczby token√≥w.
    """
    # Sortuj wyniki od najmniejszej do najwiƒôkszej liczby token√≥w
    sorted_results = sorted(results.items(), key=lambda x: x[1])

    # Znajd≈∫ maksymalnƒÖ warto≈õƒá dla skalowania s≈Çupk√≥w
    max_tokens = max(results.values())
    max_name_len = max(len(name) for name in results.keys())

    # Szeroko≈õƒá s≈Çupka (dostosuj do szeroko≈õci terminalu)
    bar_width = 50

    # Przygotuj zawarto≈õƒá panelu
    lines = []

    for i, (name, tokens) in enumerate(sorted_results):
        # Ikona medalu dla top 3
        if i == 0:
            medal = "ü•á"
        elif i == 1:
            medal = "ü•à"
        elif i == 2:
            medal = "ü•â"
        else:
            medal = "  "

        # Oblicz d≈Çugo≈õƒá s≈Çupka
        bar_length = int((tokens / max_tokens) * bar_width)
        bar = "‚ñì" * bar_length + "‚ñë" * (bar_width - bar_length)

        # Formatuj liczbƒô token√≥w z separatorem tysiƒôcy
        tokens_formatted = f"{tokens:,}".replace(",", ",")

        # Wyr√≥wnaj nazwƒô tokenizera
        name_padded = name.ljust(max_name_len)

        lines.append(f"{medal} {name_padded} {bar} {tokens_formatted:>10} token√≥w")

    content = "\n".join(lines)

    # Wy≈õwietl panel
    panel = Panel(
        content,
        title=f"üìä {text_name}",
        border_style="cyan"
    )
    console.print(panel)
    console.print()


def main():
    console.print("\n[bold cyan]‚ïê‚ïê‚ïê POR√ìWNANIE TOKENIZER√ìW ‚ïê‚ïê‚ïê[/bold cyan]\n")

    # Za≈Çaduj wszystkie tokenizery BPE
    tokenizers = {}
    for name, path in BPE_TOKENIZERS.items():
        tokenizer = load_bpe_tokenizer(path)
        if tokenizer:
            tokenizers[name] = ("bpe", tokenizer)
            console.print(f"[green]‚úì[/green] Za≈Çadowano: {name}")
        else:
            console.print(f"[yellow]‚ö†[/yellow] Brak tokenizera: {name}")

    # Za≈Çaduj tokenizer Herbert
    herbert = load_herbert_tokenizer(HERBERT_DIR)
    if herbert:
        tokenizers["herbert"] = ("herbert", herbert)
        console.print(f"[green]‚úì[/green] Za≈Çadowano: herbert")
    else:
        console.print(f"[yellow]‚ö†[/yellow] Brak tokenizera: herbert (uruchom download-herbert.py)")

    console.print()

    # Dla ka≈ºdego tekstu testowego
    for text_name, text_path in TEST_TEXTS.items():
        if not text_path.exists():
            console.print(f"[red]‚úó[/red] Brak pliku: {text_path}")
            continue

        text = load_text(text_path)
        console.print(f"[dim]Przetwarzanie: {text_name} ({len(text):,} znak√≥w)[/dim]")

        # Zlicz tokeny dla ka≈ºdego tokenizera
        results = {}
        for name, (tokenizer_type, tokenizer) in tokenizers.items():
            if tokenizer_type == "bpe":
                token_count = count_tokens_bpe(tokenizer, text)
            else:  # herbert
                token_count = count_tokens_herbert(tokenizer, text)
            results[name] = token_count

        # Wizualizuj wyniki
        visualize_results(text_name, results)

    # Podsumowanie
    console.print("[bold cyan]‚ïê‚ïê‚ïê PODSUMOWANIE ‚ïê‚ïê‚ïê[/bold cyan]\n")
    console.print("üéØ [bold]Najefektywniejszy tokenizer[/bold] to ten, kt√≥ry produkuje [green]najmniej token√≥w[/green].")
    console.print("   W≈Çasne tokenizery powinny byƒá najlepsze dla tekst√≥w z ich korpusu treningowego.")
    console.print("   Bielik v3 powinien byƒá lepszy od v1/v2 dla polskiego tekstu.\n")


if __name__ == "__main__":
    main()
