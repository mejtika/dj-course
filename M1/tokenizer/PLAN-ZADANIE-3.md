# ðŸ“š Plan pracy domowej: Zadanie 3 - Budowa wÅ‚asnych tokenizerÃ³w BPE

## Cel zadania
Stworzenie wÅ‚asnych tokenizerÃ³w BPE na podstawie rÃ³Å¼nych korpusÃ³w polskich tekstÃ³w, pobranie tokenizera z HuggingFace (`allegro/herbert-base-cased`) i przeprowadzenie analizy porÃ³wnawczej efektywnoÅ›ci tokenizacji.

---

## ðŸ”§ Krok 0: Konfiguracja Å›rodowiska

### PrzejdÅº do folderu projektu
```bash
cd M1/tokenizer
```

### UtwÃ³rz wirtualne Å›rodowisko Python
```bash
python3 -m venv venv
source venv/bin/activate   # macOS/Linux
```

### Zainstaluj wymagane pakiety
```bash
pip install -r requirements.txt
```

Plik `requirements.txt` zawiera:
- `tokenizers` - biblioteka HuggingFace do budowania tokenizerÃ³w BPE
- `transformers` - do pobierania tokenizerÃ³w z HuggingFace (Herbert)
- `rich` - do wizualizacji wynikÃ³w w terminalu

### SprawdÅº dostÄ™pnoÅ›Ä‡ korpusÃ³w
```bash
python corpora.py
```
Upewnij siÄ™, Å¼e foldery `korpus-wolnelektury/*.txt` i `korpus-nkjp/output/*.txt` zawierajÄ… pliki.

---

## ðŸ“ Krok 1: Zbuduj 4 wÅ‚asne tokenizery

Skrypt `tokenizer-build.py` zostaÅ‚ zdynamizowany i przyjmuje argumenty CLI:
- `--corpus` - nazwa korpusu (`PAN_TADEUSZ`, `WOLNELEKTURY`, `NKJP`, `ALL`)
- `--output` - Å›cieÅ¼ka wyjÅ›ciowa dla tokenizera
- `--vocab-size` - rozmiar sÅ‚ownika (domyÅ›lnie 32000)

### Metoda 1: UÅ¼yj skryptu automatycznego
```bash
chmod +x build-all-tokenizers.sh
./build-all-tokenizers.sh
```

### Metoda 2: Uruchom rÄ™cznie kaÅ¼dy tokenizer
```bash
# 1. Pan Tadeusz (12 ksiÄ…g)
python tokenizer-build.py --corpus PAN_TADEUSZ --output tokenizers/tokenizer-pan-tadeusz.json

# 2. Wolne Lektury (caÅ‚y korpus)
python tokenizer-build.py --corpus WOLNELEKTURY --output tokenizers/tokenizer-wolnelektury.json

# 3. NKJP (Narodowy Korpus JÄ™zyka Polskiego)
python tokenizer-build.py --corpus NKJP --output tokenizers/tokenizer-nkjp.json

# 4. Wszystkie korpusy razem
python tokenizer-build.py --corpus ALL --output tokenizers/tokenizer-all-corpora.json
```

### Na co zwrÃ³ciÄ‡ uwagÄ™:
- **WiÄ™kszy korpus** = dÅ‚uÅ¼szy czas treningu, ale lepsze pokrycie sÅ‚ownictwa
- **min_frequency=2** - tokeny wystÄ™pujÄ…ce tylko raz sÄ… ignorowane
- Obserwuj liczbÄ™ plikÃ³w i czas treningu

---

## ðŸŒ Krok 2: Pobierz tokenizer Herbert z HuggingFace

Herbert (`allegro/herbert-base-cased`) to polski model jÄ™zykowy uÅ¼ywajÄ…cy WordPiece (nie BPE).

```bash
python download-herbert.py
```

Tokenizer zostanie zapisany do `tokenizers/herbert/`.

**Uwaga:** Herbert wymaga innej metody Å‚adowania niÅ¼ tokenizery BPE:
```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("tokenizers/herbert")
```

---

## ðŸ”½ Krok 3: Pobierz tokenizery Bielik (jeÅ›li brakuje)

JeÅ›li pliki `bielik-v1/v2/v3-tokenizer.json` nie istniejÄ…:

```bash
python download-bielik.py
```

**Uwaga:** MoÅ¼e byÄ‡ wymagana akceptacja [terms of use](https://bielik.ai/terms/) na HuggingFace.

---

## ðŸ“Š Krok 4: Uruchom porÃ³wnanie z wizualizacjÄ…

```bash
python tokenizer-compare.py
```

### Co robi skrypt:
1. Åaduje wszystkie dostÄ™pne tokenizery (8 Å‚Ä…cznie):
   - 3 bielikowe: `bielik-v1`, `bielik-v2`, `bielik-v3`
   - 1 herbert: `tokenizers/herbert/`
   - 4 wÅ‚asne: `pan-tadeusz`, `wolnelektury`, `nkjp`, `all-corpora`

2. Tokenizuje 3 teksty testowe:
   - `Pan Tadeusz KsiÄ™ga 1` (polski, z wolnelektury)
   - `Pickwick Papers` (angielski, Gutenberg)
   - `Fryderyk Chopin` (polski, Wikipedia)

3. WyÅ›wietla wizualizacjÄ™ w terminalu z:
   - Poziomymi sÅ‚upkami posortowanymi od najlepszego
   - Medalami ðŸ¥‡ðŸ¥ˆðŸ¥‰ dla top 3
   - LiczbÄ… tokenÃ³w po prawej stronie

---

## ðŸ§ª Krok 5: Eksperymentuj z vocab_size

SprawdÅº wpÅ‚yw rozmiaru sÅ‚ownika na efektywnoÅ›Ä‡ tokenizacji:

```bash
# Mniejszy sÅ‚ownik (16k)
python tokenizer-build.py --corpus ALL --output tokenizers/tokenizer-all-16k.json --vocab-size 16000

# WiÄ™kszy sÅ‚ownik (64k)
python tokenizer-build.py --corpus ALL --output tokenizers/tokenizer-all-64k.json --vocab-size 64000
```

### Dodaj nowe tokenizery do porÃ³wnania:
Edytuj `tokenizer-compare.py` i dodaj nowe wpisy do sÅ‚ownika `BPE_TOKENIZERS`.

---

## ðŸ“ˆ Krok 6: Wnioski do wyciÄ…gniÄ™cia

Po uruchomieniu porÃ³wnania odpowiedz na pytania:

### 1. KtÃ³ry tokenizer byÅ‚ najefektywniejszy dla kaÅ¼dego tekstu?

| Tekst | Spodziewany najlepszy tokenizer |
|-------|--------------------------------|
| Pan Tadeusz KsiÄ™ga 1 | `pan-tadeusz` (treninowany na tym korpusie) |
| Pickwick Papers (ang.) | `bielik-v1/v2` (majÄ… angielski w sÅ‚owniku) |
| Fryderyk Chopin (pl) | `nkjp` lub `all-corpora` |

### 2. Jak wypada Bielik v3 vs v1/v2?
- Bielik v3 powinien byÄ‡ bardziej efektywny dla polskiego tekstu
- Bielik v1/v2 lepszy dla angielskiego (oparty na Mistral)

### 3. Jak vocab_size wpÅ‚ywa na liczbÄ™ tokenÃ³w?
- **WiÄ™kszy sÅ‚ownik** â†’ mniej tokenÃ³w (dÅ‚uÅ¼sze segmenty)
- **Mniejszy sÅ‚ownik** â†’ wiÄ™cej tokenÃ³w (krÃ³tsze segmenty)
- Istnieje punkt nasycenia - po pewnym rozmiarze przyrosty malejÄ…

### 4. Dlaczego wÅ‚asny tokenizer jest najefektywniejszy dla "swojego" tekstu?
- Tokenizer uczy siÄ™ czÄ™stych wzorcÃ³w z korpusu treningowego
- Tekst z tego samego ÅºrÃ³dÅ‚a zawiera te same wzorce
- To potwierdza, Å¼e tokenizery powinny byÄ‡ dostosowane do domeny

---

## ðŸ“ Struktura plikÃ³w po wykonaniu zadania

```
M1/tokenizer/
â”œâ”€â”€ tokenizer-build.py          # Zdynamizowany skrypt budowania
â”œâ”€â”€ tokenizer-compare.py        # PorÃ³wnanie z wizualizacjÄ…
â”œâ”€â”€ download-herbert.py         # Pobieranie Herbert z HF
â”œâ”€â”€ download-bielik.py          # Pobieranie Bielik z HF
â”œâ”€â”€ build-all-tokenizers.sh     # Skrypt automatyczny
â”œâ”€â”€ corpora.py                  # Utils do korpusÃ³w
â”œâ”€â”€ requirements.txt            # ZaleÅ¼noÅ›ci
â”œâ”€â”€ venv/                       # Åšrodowisko wirtualne
â””â”€â”€ tokenizers/
    â”œâ”€â”€ bielik-v1-tokenizer.json
    â”œâ”€â”€ bielik-v2-tokenizer.json
    â”œâ”€â”€ bielik-v3-tokenizer.json
    â”œâ”€â”€ tokenizer-pan-tadeusz.json
    â”œâ”€â”€ tokenizer-wolnelektury.json
    â”œâ”€â”€ tokenizer-nkjp.json
    â”œâ”€â”€ tokenizer-all-corpora.json
    â””â”€â”€ herbert/
        â”œâ”€â”€ tokenizer.json
        â””â”€â”€ tokenizer_config.json
```

---

## ðŸŽ¯ Podsumowanie

1. **Najefektywniejszy tokenizer** = najmniej tokenÃ³w dla danego tekstu
2. **WÅ‚asne tokenizery** sÄ… najlepsze dla tekstÃ³w z ich korpusu treningowego
3. **Bielik v3** > v1/v2 dla polskiego (nowszy, lepiej zoptymalizowany)
4. **Herbert** (WordPiece) dobrze radzi sobie z polskim
5. **vocab_size** ma znaczÄ…cy wpÅ‚yw - eksperymentuj!

---

## ðŸ”— Przydatne linki

- [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/)
- [Bielik Terms of Use](https://bielik.ai/terms/)
- [Herbert na HuggingFace](https://huggingface.co/allegro/herbert-base-cased)
- [Badanie o tokenizacji (arXiv)](https://arxiv.org/pdf/2503.01996)
