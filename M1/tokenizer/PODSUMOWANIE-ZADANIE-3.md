# ğŸ“– Podsumowanie Zadania 3: Tokenizery BPE

## Spis treÅ›ci
1. [Podstawowe pojÄ™cia](#-podstawowe-pojÄ™cia)
2. [Co to jest tokenizer?](#-co-to-jest-tokenizer)
3. [Algorytm BPE](#-algorytm-bpe-byte-pair-encoding)
4. [OmÃ³wienie plikÃ³w projektu](#-omÃ³wienie-plikÃ³w-projektu)
5. [SkÅ‚adnia Pythona - najwaÅ¼niejsze elementy](#-skÅ‚adnia-pythona---najwaÅ¼niejsze-elementy)
6. [Wnioski z zadania](#-wnioski-z-zadania)

---

## ğŸ§  Podstawowe pojÄ™cia

### Token
**Token** to najmniejsza jednostka tekstu, ktÃ³rÄ… model jÄ™zykowy (LLM) przetwarza. MoÅ¼e to byÄ‡:
- CaÅ‚e sÅ‚owo: `"hello"` â†’ `["hello"]`
- CzÄ™Å›Ä‡ sÅ‚owa: `"unhappiness"` â†’ `["un", "happiness"]`
- Pojedynczy znak: `"ğŸ‰"` â†’ `["ğŸ‰"]`

### Tokenizacja
**Tokenizacja** to proces dzielenia tekstu na tokeny. Jest to pierwszy krok w przetwarzaniu tekstu przez LLM.

```
"Litwo! Ojczyzno moja!" â†’ ["Li", "two", "!", "Ojczy", "zno", "moja", "!"]
```

### SÅ‚ownik (Vocabulary)
**SÅ‚ownik** to zbiÃ³r wszystkich znanych tokenÃ³w. Rozmiar sÅ‚ownika (np. 32 000) okreÅ›la ile rÃ³Å¼nych tokenÃ³w moÅ¼e rozpoznaÄ‡ tokenizer.

### Korpus
**Korpus** to zbiÃ³r tekstÃ³w uÅ¼ywanych do trenowania tokenizera. Im wiÄ™kszy i bardziej rÃ³Å¼norodny korpus, tym lepszy tokenizer.

---

## ğŸ”¤ Co to jest tokenizer?

Tokenizer to narzÄ™dzie, ktÃ³re:
1. **Dzieli tekst** na mniejsze jednostki (tokeny)
2. **Mapuje tokeny na liczby** (ID) - bo komputery rozumiejÄ… tylko liczby
3. **Odwraca proces** - zamienia ID z powrotem na tekst

### PrzykÅ‚ad tokenizacji

```python
tekst = "Litwo! Ojczyzno moja!"

# Po tokenizacji:
tokeny = ["Li", "two", "!", "Ojczy", "zno", "moja", "!"]
ids = [496, 521, 5, 1272, 482, 1850, 5]
```

### Dlaczego to waÅ¼ne?

| Aspekt | WpÅ‚yw |
|--------|-------|
| **Koszty API** | PÅ‚acisz za tokeny, nie znaki! Lepszy tokenizer = mniej tokenÃ³w = niÅ¼sze koszty |
| **JakoÅ›Ä‡ modelu** | Tokenizer wpÅ‚ywa na to, jak model "rozumie" tekst |
| **JÄ™zyk** | Tokenizer wytrenowany na polskim tekÅ›cie lepiej radzi sobie z polszczyznÄ… |

---

## ğŸ”§ Algorytm BPE (Byte Pair Encoding)

BPE to najpopularniejszy algorytm tokenizacji uÅ¼ywany przez GPT, Bielik, Mistral i inne modele.

### Jak dziaÅ‚a BPE?

1. **Start**: Zacznij od pojedynczych znakÃ³w jako tokenÃ³w
2. **Zlicz pary**: ZnajdÅº najczÄ™Å›ciej wystÄ™pujÄ…cÄ… parÄ™ znakÃ³w
3. **PoÅ‚Ä…cz**: ZamieÅ„ tÄ™ parÄ™ na nowy token
4. **PowtÃ³rz**: WrÃ³Ä‡ do kroku 2 aÅ¼ osiÄ…gniesz Å¼Ä…dany rozmiar sÅ‚ownika

### PrzykÅ‚ad krok po kroku

```
Korpus: "low lower lowest"

Krok 1: PoczÄ…tkowe tokeny
  l o w _ l o w e r _ l o w e s t
  
Krok 2: NajczÄ™stsza para = "lo" (wystÄ™puje 3x)
  Nowy token: "lo"
  lo w _ lo w e r _ lo w e s t

Krok 3: NajczÄ™stsza para = "low" (wystÄ™puje 3x)
  Nowy token: "low"
  low _ low e r _ low e s t

... i tak dalej
```

### BPE vs WordPiece

| BPE | WordPiece |
|-----|-----------|
| UÅ¼ywany przez: GPT, Bielik, Mistral | UÅ¼ywany przez: BERT, Herbert |
| ÅÄ…czy najczÄ™stsze pary | Maksymalizuje likelihood |
| Prosty algorytm | Bardziej zÅ‚oÅ¼ony |

---

## ğŸ“ OmÃ³wienie plikÃ³w projektu

### 1. `corpora.py` - ZarzÄ…dzanie korpusami

Ten plik definiuje **skÄ…d braÄ‡ teksty do trenowania** tokenizera.

```python
# IMPORT: Åadowanie bibliotek
import glob                    # Do wyszukiwania plikÃ³w po wzorcu (*.txt)
from pathlib import Path       # Nowoczesna obsÅ‚uga Å›cieÅ¼ek plikÃ³w

# SÅOWNIKI: Definiowanie Å›cieÅ¼ek do korpusÃ³w
CORPORA_DIRS = {
    "NKJP": Path("../korpus-nkjp/output"),      # Narodowy Korpus JÄ™zyka Polskiego
    "WOLNELEKTURY": Path("../korpus-wolnelektury"),  # Wolne Lektury
}

# SÅOWNIKI: Listy plikÃ³w dla kaÅ¼dego korpusu
CORPORA_FILES = {
    "NKJP": list(CORPORA_DIRS["NKJP"].glob("*.txt")),           # Wszystkie .txt z NKJP
    "WOLNELEKTURY": list(CORPORA_DIRS["WOLNELEKTURY"].glob("*.txt")),  # Wszystkie z Wolnych Lektur
    "PAN_TADEUSZ": list(CORPORA_DIRS["WOLNELEKTURY"].glob("pan-tadeusz-ksiega-*.txt")),  # Tylko Pan Tadeusz
}

# LIST COMPREHENSION: Tworzenie listy ALL bez duplikatÃ³w
KEYS_WITHOUT_PAN_TADEUSZ = [key for key in CORPORA_FILES.keys() if key != "PAN_TADEUSZ"]
CORPORA_FILES["ALL"] = [
    FILE for key in KEYS_WITHOUT_PAN_TADEUSZ for FILE in CORPORA_FILES[key]
]

# FUNKCJA: Pobieranie plikÃ³w z korpusu po wzorcu
def get_corpus_file(corpus_name: str, glob_pattern: str) -> Path:
    if corpus_name not in CORPORA_FILES:
        raise ValueError(f"Corpus {corpus_name} not found")  # RzuÄ‡ bÅ‚Ä…d jeÅ›li nie znaleziono
    return list(CORPORA_DIRS[corpus_name].glob(glob_pattern))

# ENTRY POINT: Kod uruchamiany tylko gdy plik jest wykonywany bezpoÅ›rednio
if __name__ == "__main__":    
    print("\ncorpora (total files):")
    for corpus_name, corpus_files in CORPORA_FILES.items():
        print(f"{corpus_name}: {len(corpus_files)}")  # f-string do formatowania
```

**Kluczowe elementy skÅ‚adni:**
- `Path()` - obiekt reprezentujÄ…cy Å›cieÅ¼kÄ™ do pliku/folderu
- `.glob("*.txt")` - wyszukuje pliki pasujÄ…ce do wzorca
- `list()` - konwertuje generator na listÄ™
- `[x for x in lista]` - list comprehension (skrÃ³cona pÄ™tla tworzÄ…ca listÄ™)
- `if __name__ == "__main__"` - kod uruchamiany tylko gdy plik jest gÅ‚Ã³wnym skryptem

---

### 2. `tokenizer-build.py` - Budowanie tokenizera

Ten plik **trenuje nowy tokenizer BPE** na podstawie wybranego korpusu.

```python
# IMPORTY
import argparse                              # Parsowanie argumentÃ³w z linii komend
from tokenizers import Tokenizer             # GÅ‚Ã³wna klasa tokenizera
from tokenizers.models import BPE            # Model BPE
from tokenizers.trainers import BpeTrainer   # Trener dla BPE
from tokenizers.pre_tokenizers import Whitespace  # WstÄ™pny podziaÅ‚ po spacjach
from corpora import CORPORA_FILES            # Import z naszego pliku corpora.py

# FUNKCJA z type hints (podpowiedziami typÃ³w)
def build_tokenizer(files: list[str], output_path: str, vocab_size: int = 32000):
    """
    Docstring - opis funkcji (pojawia siÄ™ w dokumentacji).
    Buduje tokenizer BPE na podstawie podanych plikÃ³w i zapisuje go do output_path.
    """
    # Krok 1: Inicjalizacja tokenizera z modelem BPE
    tokenizer = Tokenizer(BPE(unk_token="[UNK]"))  # [UNK] = nieznany token

    # Krok 2: Pre-tokenizer dzieli tekst po spacjach PRZED gÅ‚Ã³wnÄ… tokenizacjÄ…
    tokenizer.pre_tokenizer = Whitespace()

    # Krok 3: Konfiguracja trenera
    trainer = BpeTrainer(
        special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"],  # Tokeny specjalne
        vocab_size=vocab_size,     # Rozmiar sÅ‚ownika (domyÅ›lnie 32000)
        min_frequency=2            # Token musi wystÄ…piÄ‡ min. 2 razy
    )

    # Krok 4: Trening tokenizera na plikach
    tokenizer.train(files, trainer=trainer)

    # Krok 5: Zapis do pliku JSON
    tokenizer.save(output_path)
    
    return tokenizer  # ZwrÃ³Ä‡ wytrenowany tokenizer


def main():
    # ARGPARSE: Definiowanie argumentÃ³w CLI
    parser = argparse.ArgumentParser(
        description="Budowanie tokenizera BPE na podstawie korpusu tekstowego"
    )
    
    # Argument --corpus (wymagany)
    parser.add_argument(
        "--corpus",                           # Nazwa argumentu
        type=str,                             # Typ: string
        required=True,                        # Wymagany
        choices=list(CORPORA_FILES.keys()),   # Dozwolone wartoÅ›ci
        help=f"Nazwa korpusu: {list(CORPORA_FILES.keys())}"  # Pomoc
    )
    
    # Argument --output (wymagany)
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="ÅšcieÅ¼ka wyjÅ›ciowa dla tokenizera"
    )
    
    # Argument --vocab-size (opcjonalny, domyÅ›lnie 32000)
    parser.add_argument(
        "--vocab-size",
        type=int,
        default=32000,                        # WartoÅ›Ä‡ domyÅ›lna
        help="Rozmiar sÅ‚ownika (domyÅ›lnie 32000)"
    )

    # Parsuj argumenty z linii komend
    args = parser.parse_args()

    # Pobierz pliki dla wybranego korpusu
    files = [str(f) for f in CORPORA_FILES[args.corpus]]  # Konwersja Path â†’ str

    # Buduj tokenizer
    tokenizer = build_tokenizer(files, args.output, args.vocab_size)

    # Test tokenizera
    test_texts = ["Litwo! Ojczyzno moja!"]
    for txt in test_texts:
        encoded = tokenizer.encode(txt)       # Tokenizacja
        print(f"Tokeny: {encoded.tokens}")    # Lista tokenÃ³w
        print(f"IDs: {encoded.ids}")          # Lista ID


# ENTRY POINT
if __name__ == "__main__":
    main()
```

**WywoÅ‚anie z CLI:**
```bash
python tokenizer-build.py --corpus PAN_TADEUSZ --output tokenizers/tokenizer-pan-tadeusz.json --vocab-size 32000
```

---

### 3. `tokenizer-compare.py` - PorÃ³wnanie tokenizerÃ³w

Ten plik **porÃ³wnuje efektywnoÅ›Ä‡** rÃ³Å¼nych tokenizerÃ³w i **wizualizuje wyniki**.

```python
# IMPORTY
from pathlib import Path
from tokenizers import Tokenizer              # Dla tokenizerÃ³w BPE
from transformers import AutoTokenizer        # Dla tokenizerÃ³w z HuggingFace (Herbert)
from rich.console import Console              # Kolorowy output w terminalu
from rich.panel import Panel                  # Ramki w terminalu
from typing import Optional                   # Typy opcjonalne (moÅ¼e byÄ‡ None)

# Obiekt do wypisywania kolorowego tekstu
console = Console()

# STAÅE: ÅšcieÅ¼ki do tokenizerÃ³w
TOKENIZERS_DIR = Path("tokenizers")

# SÅOWNIK: Mapowanie nazwa â†’ Å›cieÅ¼ka dla tokenizerÃ³w BPE
BPE_TOKENIZERS = {
    "bielik-v1": TOKENIZERS_DIR / "bielik-v1-tokenizer.json",   # / Å‚Ä…czy Å›cieÅ¼ki
    "bielik-v2": TOKENIZERS_DIR / "bielik-v2-tokenizer.json",
    "bielik-v3": TOKENIZERS_DIR / "bielik-v3-tokenizer.json",
    "pan-tadeusz": TOKENIZERS_DIR / "tokenizer-pan-tadeusz.json",
    "wolnelektury": TOKENIZERS_DIR / "tokenizer-wolnelektury.json",
    "nkjp": TOKENIZERS_DIR / "tokenizer-nkjp.json",
    "all-corpora": TOKENIZERS_DIR / "tokenizer-all-corpora.json",
}

# SÅOWNIK: Teksty testowe
TEST_TEXTS = {
    "Pan Tadeusz KsiÄ™ga 1": Path("../korpus-wolnelektury/pan-tadeusz-ksiega-1.txt"),
    "Pickwick Papers": Path("../korpus-mini/the-pickwick-papers-gutenberg.txt"),
    "Fryderyk Chopin": Path("../korpus-mini/fryderyk-chopin-wikipedia.txt"),
}


# FUNKCJA z Optional (moÅ¼e zwrÃ³ciÄ‡ None)
def load_bpe_tokenizer(path: Path) -> Optional[Tokenizer]:
    """Åaduje tokenizer BPE z pliku JSON."""
    if not path.exists():         # SprawdÅº czy plik istnieje
        return None               # ZwrÃ³Ä‡ None jeÅ›li nie
    try:
        return Tokenizer.from_file(str(path))  # ZaÅ‚aduj tokenizer
    except Exception as e:        # ZÅ‚ap dowolny bÅ‚Ä…d
        console.print(f"[red]BÅ‚Ä…d: {e}[/red]")  # WyÅ›wietl na czerwono
        return None


def count_tokens_bpe(tokenizer: Tokenizer, text: str) -> int:
    """Zlicza tokeny dla tokenizera BPE."""
    encoded = tokenizer.encode(text)  # Tokenizuj tekst
    return len(encoded.ids)           # ZwrÃ³Ä‡ liczbÄ™ tokenÃ³w


def load_text(path: Path) -> str:
    """Wczytuje tekst z pliku."""
    with open(path, 'r', encoding='utf-8') as f:  # Context manager - auto-zamykanie
        return f.read()                            # Wczytaj caÅ‚Ä… zawartoÅ›Ä‡


def visualize_results(text_name: str, results: dict[str, int]):
    """Wizualizuje wyniki jako poziome sÅ‚upki."""
    
    # Sortuj wyniki od najmniejszej liczby tokenÃ³w (najlepszy = najmniej)
    sorted_results = sorted(results.items(), key=lambda x: x[1])
    
    # ZnajdÅº maksimum do skalowania sÅ‚upkÃ³w
    max_tokens = max(results.values())
    
    # Przygotuj linie do wyÅ›wietlenia
    lines = []
    for i, (name, tokens) in enumerate(sorted_results):  # enumerate daje indeks i wartoÅ›Ä‡
        # Medale dla top 3
        if i == 0:
            medal = "ğŸ¥‡"
        elif i == 1:
            medal = "ğŸ¥ˆ"
        elif i == 2:
            medal = "ğŸ¥‰"
        else:
            medal = "  "
        
        # Oblicz dÅ‚ugoÅ›Ä‡ sÅ‚upka proporcjonalnie
        bar_length = int((tokens / max_tokens) * 50)
        bar = "â–“" * bar_length + "â–‘" * (50 - bar_length)
        
        # Formatuj liczbÄ™ z separatorem tysiÄ™cy
        tokens_formatted = f"{tokens:,}"  # :, dodaje separator tysiÄ™cy
        
        lines.append(f"{medal} {name.ljust(15)} {bar} {tokens_formatted:>10} tokenÃ³w")
    
    # WyÅ›wietl w ramce
    panel = Panel("\n".join(lines), title=f"ğŸ“Š {text_name}")
    console.print(panel)


def main():
    # ZaÅ‚aduj wszystkie tokenizery
    tokenizers = {}
    for name, path in BPE_TOKENIZERS.items():  # Iteruj po sÅ‚owniku
        tokenizer = load_bpe_tokenizer(path)
        if tokenizer:                          # JeÅ›li udaÅ‚o siÄ™ zaÅ‚adowaÄ‡
            tokenizers[name] = ("bpe", tokenizer)  # Dodaj do sÅ‚ownika
    
    # Dla kaÅ¼dego tekstu testowego
    for text_name, text_path in TEST_TEXTS.items():
        text = load_text(text_path)            # Wczytaj tekst
        
        # Zlicz tokeny dla kaÅ¼dego tokenizera
        results = {}
        for name, (tokenizer_type, tokenizer) in tokenizers.items():
            results[name] = count_tokens_bpe(tokenizer, text)
        
        # Wizualizuj wyniki
        visualize_results(text_name, results)


if __name__ == "__main__":
    main()
```

---

### 4. `download-herbert.py` - Pobieranie Herbert

```python
from transformers import AutoTokenizer  # Biblioteka HuggingFace
import os

OUTPUT_DIR = "tokenizers/herbert"

def main():
    # Pobierz tokenizer z HuggingFace (wymaga internetu)
    tokenizer = AutoTokenizer.from_pretrained("allegro/herbert-base-cased")
    
    # UtwÃ³rz folder jeÅ›li nie istnieje
    os.makedirs(OUTPUT_DIR, exist_ok=True)  # exist_ok=True nie rzuca bÅ‚Ä™du jeÅ›li istnieje
    
    # Zapisz tokenizer lokalnie
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # Test
    encoded = tokenizer.encode("Test tekstu")
    tokens = tokenizer.convert_ids_to_tokens(encoded)  # ID â†’ tokeny
    print(f"Tokeny: {tokens}")


if __name__ == "__main__":
    main()
```

---

### 5. `download-bielik.py` - Pobieranie Bielik

```python
import os
import urllib.request  # Biblioteka do pobierania plikÃ³w z internetu

# SÅ‚ownik z URLami do tokenizerÃ³w
TOKENIZERS = {
    "bielik-v1": "https://huggingface.co/.../tokenizer.json",
    "bielik-v2": "https://huggingface.co/.../tokenizer.json",
    "bielik-v3": "https://huggingface.co/.../tokenizer.json",
}

def main():
    os.makedirs("tokenizers", exist_ok=True)
    
    for name, url in TOKENIZERS.items():  # Iteruj po sÅ‚owniku
        output_path = os.path.join("tokenizers", f"{name}-tokenizer.json")  # ZÅ‚Ä…cz Å›cieÅ¼kÄ™
        
        if os.path.exists(output_path):   # SprawdÅº czy plik juÅ¼ istnieje
            print(f"âœ“ {name} juÅ¼ istnieje")
            continue                       # PomiÅ„ do nastÄ™pnej iteracji
        
        try:
            urllib.request.urlretrieve(url, output_path)  # Pobierz plik
            print(f"âœ“ Zapisano: {output_path}")
        except Exception as e:
            print(f"âœ— BÅ‚Ä…d: {e}")


if __name__ == "__main__":
    main()
```

---

## ğŸ SkÅ‚adnia Pythona - najwaÅ¼niejsze elementy

### 1. Importy

```python
import os                          # Import caÅ‚ego moduÅ‚u
from pathlib import Path           # Import konkretnej klasy z moduÅ‚u
from tokenizers import Tokenizer   # Import z zewnÄ™trznej biblioteki
```

### 2. Zmienne i typy

```python
# Python nie wymaga deklaracji typÃ³w, ale moÅ¼na je dodaÄ‡ (type hints)
name: str = "tekst"                # String (napis)
count: int = 42                    # Integer (liczba caÅ‚kowita)
price: float = 3.14                # Float (liczba zmiennoprzecinkowa)
is_active: bool = True             # Boolean (prawda/faÅ‚sz)
items: list = [1, 2, 3]            # Lista (tablica)
config: dict = {"key": "value"}    # SÅ‚ownik (mapa klucz-wartoÅ›Ä‡)
```

### 3. Funkcje

```python
# Definicja funkcji
def nazwa_funkcji(argument1: str, argument2: int = 10) -> str:
    """Docstring - opis funkcji."""
    wynik = argument1 + str(argument2)
    return wynik

# WywoÅ‚anie
rezultat = nazwa_funkcji("test", 5)  # "test5"
```

### 4. PÄ™tle

```python
# PÄ™tla for po liÅ›cie
for item in [1, 2, 3]:
    print(item)

# PÄ™tla for ze sÅ‚ownikiem
for key, value in {"a": 1, "b": 2}.items():
    print(f"{key}: {value}")

# enumerate - indeks + wartoÅ›Ä‡
for i, item in enumerate(["a", "b", "c"]):
    print(f"{i}: {item}")  # 0: a, 1: b, 2: c
```

### 5. List Comprehension (skrÃ³cone pÄ™tle)

```python
# Zamiast:
wynik = []
for x in [1, 2, 3]:
    wynik.append(x * 2)

# MoÅ¼esz napisaÄ‡:
wynik = [x * 2 for x in [1, 2, 3]]  # [2, 4, 6]

# Z warunkiem:
parzyste = [x for x in [1, 2, 3, 4] if x % 2 == 0]  # [2, 4]
```

### 6. F-stringi (formatowanie tekstu)

```python
name = "Jan"
age = 25
print(f"Mam na imiÄ™ {name} i mam {age} lat")

# Formatowanie liczb
tokens = 12345
print(f"{tokens:,}")      # "12,345" (separator tysiÄ™cy)
print(f"{tokens:>10}")    # "     12345" (wyrÃ³wnanie do prawej, 10 znakÃ³w)
```

### 7. Context Manager (with)

```python
# Automatycznie zamyka plik po zakoÅ„czeniu bloku
with open("plik.txt", "r", encoding="utf-8") as f:
    content = f.read()
# Tutaj plik jest juÅ¼ zamkniÄ™ty
```

### 8. Try/Except (obsÅ‚uga bÅ‚Ä™dÃ³w)

```python
try:
    wynik = 10 / 0  # Spowoduje bÅ‚Ä…d
except ZeroDivisionError:
    print("Nie moÅ¼na dzieliÄ‡ przez zero!")
except Exception as e:  # ZÅ‚ap dowolny bÅ‚Ä…d
    print(f"BÅ‚Ä…d: {e}")
```

### 9. Entry Point

```python
if __name__ == "__main__":
    main()
```
Ten blok uruchamia siÄ™ **tylko gdy plik jest wykonywany bezpoÅ›rednio** (`python plik.py`), a nie gdy jest importowany.

---

## ğŸ“Š Wnioski z zadania

### 1. EfektywnoÅ›Ä‡ tokenizacji zaleÅ¼y od korpusu treningowego

| Tekst | Najlepszy tokenizer | Dlaczego? |
|-------|---------------------|-----------|
| Pan Tadeusz | `pan-tadeusz` | Treninowany na tym samym tekÅ›cie |
| Pickwick Papers (ang.) | `bielik-v1/v2` | Oparte na Mistral, majÄ… angielski |
| Fryderyk Chopin (pl) | `nkjp` lub `herbert` | Treninowane na polskim |

### 2. Bielik v3 > v1/v2 dla polskiego

Bielik v3 jest nowszy i zoptymalizowany pod kÄ…tem polszczyzny. Produkuje mniej tokenÃ³w dla polskich tekstÃ³w.

### 3. Rozmiar sÅ‚ownika (vocab_size) ma znaczenie

| vocab_size | Efekt |
|------------|-------|
| 16 000 | WiÄ™cej tokenÃ³w, mniejszy plik |
| 32 000 | DomyÅ›lny, dobry kompromis |
| 64 000 | Mniej tokenÃ³w, wiÄ™kszy plik |

### 4. WÅ‚asny tokenizer = najlepsza efektywnoÅ›Ä‡

JeÅ›li wiesz, z jakim tekstem bÄ™dziesz pracowaÄ‡, warto wytrenowaÄ‡ dedykowany tokenizer. OszczÄ™dza to tokeny (= pieniÄ…dze przy API).

---

## ğŸš€ Polecenia do uruchomienia

```bash
# 1. PrzejdÅº do folderu
cd M1/tokenizer

# 2. Aktywuj Å›rodowisko wirtualne
source venv/bin/activate

# 3. Zainstaluj zaleÅ¼noÅ›ci
pip install -r requirements.txt

# 4. Zbuduj wszystkie tokenizery
./build-all-tokenizers.sh

# 5. Pobierz Herbert
python download-herbert.py

# 6. Pobierz Bielik (jeÅ›li brakuje)
python download-bielik.py

# 7. Uruchom porÃ³wnanie z wizualizacjÄ…
python tokenizer-compare.py
```

---

## ğŸ”— Przydatne zasoby

- [Dokumentacja tokenizers (HuggingFace)](https://huggingface.co/docs/tokenizers/)
- [Badanie o tokenizacji](https://arxiv.org/pdf/2503.01996) - cytowane w zadaniu
- [Python Tutorial (W3Schools)](https://www.w3schools.com/python/)
- [Real Python - Type Hints](https://realpython.com/python-type-checking/)
