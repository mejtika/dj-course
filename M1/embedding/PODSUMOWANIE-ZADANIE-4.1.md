# ğŸ“– Podsumowanie Zadania 4.1: Embeddingi CBOW

## Spis treÅ›ci
1. [Podstawowe pojÄ™cia](#-podstawowe-pojÄ™cia)
2. [Jak dziaÅ‚a CBOW?](#-jak-dziaÅ‚a-cbow)
3. [OmÃ³wienie plikÃ³w projektu](#-omÃ³wienie-plikÃ³w-projektu)
4. [SkÅ‚adnia Pythona](#-skÅ‚adnia-pythona---kluczowe-elementy)
5. [Jak uruchomiÄ‡](#-jak-uruchomiÄ‡)
6. [Eksperymenty i wnioski](#-eksperymenty-i-wnioski)

---

## ğŸ§  Podstawowe pojÄ™cia

### Embedding (wektor osadzenia)
**Embedding** to reprezentacja sÅ‚owa (lub tokenu) jako wektora liczb. SÅ‚owa o podobnym znaczeniu majÄ… podobne wektory.

```
"krÃ³l"    â†’ [0.82, -0.15, 0.43, ...]
"ksiÄ…Å¼Ä™"  â†’ [0.78, -0.12, 0.41, ...]  â† podobny wektor!
"jabÅ‚ko"  â†’ [-0.21, 0.67, -0.33, ...]  â† inny wektor
```

### PodobieÅ„stwo kosinusowe
Miara podobieÅ„stwa miÄ™dzy dwoma wektorami. WartoÅ›Ä‡ od -1 do 1:
- **1.0** = identyczne
- **0.7+** = bardzo podobne
- **0.5+** = podobne
- **0.0** = niezaleÅ¼ne
- **-1.0** = przeciwne

### Word2Vec
Algorytm do uczenia embeddingÃ³w sÅ‚Ã³w. Dwa tryby:
- **CBOW** (Continuous Bag of Words) - przewiduje sÅ‚owo na podstawie kontekstu
- **Skip-gram** - przewiduje kontekst na podstawie sÅ‚owa

---

## ğŸ”§ Jak dziaÅ‚a CBOW?

CBOW (Continuous Bag-of-Words) to sieÄ‡ neuronowa, ktÃ³ra uczy siÄ™ przewidywaÄ‡ sÅ‚owo Å›rodkowe na podstawie sÅ‚Ã³w otaczajÄ…cych (kontekstu).

### PrzykÅ‚ad

```
Zdanie: "KrÃ³l siedziaÅ‚ na zÅ‚otym tronie"
Okno kontekstowe (window=2):

Kontekst: ["KrÃ³l", "siedziaÅ‚", "zÅ‚otym", "tronie"]
    â†“
  CBOW
    â†“
Przewidywane sÅ‚owo: "na"
```

### Parametry modelu

| Parametr | Opis | Typowe wartoÅ›ci |
|----------|------|-----------------|
| `vector_size` | Wymiar wektora embeddingu | 50-300 |
| `window` | Rozmiar okna kontekstowego | 3-10 |
| `min_count` | Min. liczba wystÄ…pieÅ„ tokenu | 1-5 |
| `epochs` | Liczba epok treningu | 10-100 |
| `sample` | PrÃ³g subsamplingu czÄ™stych sÅ‚Ã³w | 1e-3 - 1e-5 |

### WpÅ‚yw parametrÃ³w

- **WiÄ™kszy `vector_size`** â†’ lepsze reprezentacje, ale wolniejszy trening
- **WiÄ™kszy `window`** â†’ wiÄ™cej kontekstu, ale moÅ¼e rozmywaÄ‡ znaczenie
- **WiÄ™cej `epochs`** â†’ lepsze wyniki, ale istnieje punkt nasycenia
- **Mniejszy `min_count`** â†’ wiÄ™cej sÅ‚Ã³w w sÅ‚owniku, ale rzadkie sÅ‚owa majÄ… sÅ‚abe wektory

---

## ğŸ“Š SzczegÃ³Å‚owe wyjaÅ›nienie parametrÃ³w

### `vector_size` - Wymiar wektora embeddingu

**Co to jest?**
Liczba wymiarÃ³w (liczb) w wektorze reprezentujÄ…cym kaÅ¼de sÅ‚owo/token.

```
vector_size=50:   krÃ³l â†’ [0.12, -0.34, 0.56, ..., 0.78]  # 50 liczb
vector_size=100:  krÃ³l â†’ [0.12, -0.34, 0.56, ..., 0.78]  # 100 liczb
vector_size=300:  krÃ³l â†’ [0.12, -0.34, 0.56, ..., 0.78]  # 300 liczb
```

**WpÅ‚yw na wyniki:**

| vector_size | Zalety | Wady |
|-------------|--------|------|
| **50** | Szybki trening, maÅ‚y model | MoÅ¼e nie uchwyciÄ‡ subtelnych rÃ³Å¼nic znaczeniowych |
| **100** | Dobry kompromis | - |
| **200-300** | Bogate reprezentacje, lepsze dla duÅ¼ych korpusÃ³w | Wolniejszy trening, wymaga wiÄ™cej danych |
| **500+** | Teoretycznie najlepsze | Overfitting przy maÅ‚ych korpusach, bardzo wolny |

**Praktyczna zasada:**
- MaÅ‚y korpus (< 1 mln sÅ‚Ã³w): `vector_size=50-100`
- Åšredni korpus (1-10 mln sÅ‚Ã³w): `vector_size=100-200`
- DuÅ¼y korpus (> 10 mln sÅ‚Ã³w): `vector_size=200-300`

**Eksperyment:**
```bash
# PorÃ³wnaj wyniki dla rÃ³Å¼nych vector_size
python cbow-train.py --corpus ALL --tokenizer all-corpora --vector-size 50 --epochs 30
python cbow-train.py --corpus ALL --tokenizer all-corpora --vector-size 100 --epochs 30
python cbow-train.py --corpus ALL --tokenizer all-corpora --vector-size 200 --epochs 30
```

---

### `window` - Rozmiar okna kontekstowego
![img.png](img.png)

**Co to jest?**
Ile sÅ‚Ã³w po lewej i prawej stronie sÅ‚owa docelowego model bierze pod uwagÄ™ jako kontekst.

```
Zdanie: "KrÃ³l siedziaÅ‚ na zÅ‚otym tronie w zamku"
SÅ‚owo docelowe: "zÅ‚otym"

window=2: Kontekst = ["siedziaÅ‚", "na", "tronie", "w"]
window=3: Kontekst = ["KrÃ³l", "siedziaÅ‚", "na", "tronie", "w", "zamku"]
window=5: Kontekst = ["KrÃ³l", "siedziaÅ‚", "na", "tronie", "w", "zamku"] (caÅ‚e zdanie)
```

**WpÅ‚yw na wyniki:**

| window | Co uchwytuje | PrzykÅ‚ad |
|--------|--------------|----------|
| **2-3** | Relacje skÅ‚adniowe (gramatyczne) | przymiotnik-rzeczownik, czasownik-dopeÅ‚nienie |
| **5** | Relacje tematyczne (Å›redni kontekst) | Dobry kompromis dla wiÄ™kszoÅ›ci zadaÅ„ |
| **8-10** | Relacje semantyczne (szeroki kontekst) | Synonimy, sÅ‚owa z tej samej dziedziny |
| **15+** | Bardzo szeroki kontekst | MoÅ¼e rozmywaÄ‡ znaczenie, Å‚Ä…czy sÅ‚owa luÅºno powiÄ…zane |

**Praktyczna zasada:**
- Relacje gramatyczne: `window=2-3`
- OgÃ³lne podobieÅ„stwo: `window=5` (domyÅ›lne)
- Synonimy/tematy: `window=8-10`

**Wizualizacja:**
```
window=2:
    [siedziaÅ‚] [na] [ZÅOTYM] [tronie] [w]
         â†2â†’          â†2â†’
    
window=5:
[KrÃ³l] [siedziaÅ‚] [na] [ZÅOTYM] [tronie] [w] [zamku]
            â†5â†’                    â†5â†’
```

**Eksperyment:**
```bash
python cbow-train.py --corpus ALL --tokenizer all-corpora --window 3 --epochs 30
python cbow-train.py --corpus ALL --tokenizer all-corpora --window 5 --epochs 30
python cbow-train.py --corpus ALL --tokenizer all-corpora --window 8 --epochs 30
```

---

### `epochs` - Liczba epok treningu

**Co to jest?**
Ile razy model przejdzie przez caÅ‚y korpus treningowy.

```
epochs=1:  Model widzi kaÅ¼de zdanie 1 raz
epochs=10: Model widzi kaÅ¼de zdanie 10 razy
epochs=50: Model widzi kaÅ¼de zdanie 50 razy
```

**WpÅ‚yw na wyniki:**

| epochs | Efekt | Uwagi |
|--------|-------|-------|
| **5-10** | Szybki trening, podstawowe relacje | MoÅ¼e nie uchwyciÄ‡ rzadkich sÅ‚Ã³w |
| **20-30** | Dobry kompromis | Standardowe ustawienie |
| **50-100** | DokÅ‚adne reprezentacje | DÅ‚uÅ¼szy trening, ryzyko overfittingu |
| **200+** | Diminishing returns | Czas roÅ›nie, jakoÅ›Ä‡ siÄ™ stabilizuje |

**Krzywa uczenia:**
```
JakoÅ›Ä‡
  ^
  |                    ___________  â† plateau (overfitting)
  |                ___/
  |            ___/
  |        ___/
  |    ___/
  |___/
  +--------------------------------> epochs
     10   20   30   50   100
```

**Praktyczna zasada:**
- Szybki test: `epochs=10`
- Trening produkcyjny: `epochs=30-50`
- Maksymalna jakoÅ›Ä‡: `epochs=100` (ale sprawdÅº czy jest poprawa)

**Eksperyment z monitorowaniem:**
```bash
# Trenuj z rÃ³Å¼nÄ… liczbÄ… epok i porÃ³wnaj wyniki
for epochs in 10 20 30 50; do
    python cbow-train.py --corpus ALL --tokenizer all-corpora --epochs $epochs \
        --model-name cbow_epochs_$epochs
done
```

---

### `min_count` - Minimalna czÄ™stotliwoÅ›Ä‡

**Co to jest?**
SÅ‚owa wystÄ™pujÄ…ce rzadziej niÅ¼ `min_count` razy sÄ… ignorowane.

```
Korpus: "krÃ³l krÃ³l krÃ³l ksiÄ…Å¼Ä™ ksiÄ…Å¼Ä™ rycerz"
CzÄ™stotliwoÅ›ci: krÃ³l=3, ksiÄ…Å¼Ä™=2, rycerz=1

min_count=1: SÅ‚ownik = [krÃ³l, ksiÄ…Å¼Ä™, rycerz]  # 3 sÅ‚owa
min_count=2: SÅ‚ownik = [krÃ³l, ksiÄ…Å¼Ä™]           # 2 sÅ‚owa
min_count=3: SÅ‚ownik = [krÃ³l]                   # 1 sÅ‚owo
```

**WpÅ‚yw na wyniki:**

| min_count | Efekt |
|-----------|-------|
| **1** | Wszystkie sÅ‚owa, ale rzadkie majÄ… sÅ‚abe wektory |
| **2** | Filtruje literÃ³wki i bardzo rzadkie sÅ‚owa |
| **5** | Tylko "pewne" sÅ‚owa, dobre dla duÅ¼ych korpusÃ³w |
| **10+** | Tylko czÄ™ste sÅ‚owa, tracisz rzadkie ale waÅ¼ne |

**Praktyczna zasada:** `min_count=2` to bezpieczny wybÃ³r.

---

### `sample` - Subsampling czÄ™stych sÅ‚Ã³w

**Co to jest?**
PrawdopodobieÅ„stwo pominiÄ™cia bardzo czÄ™stych sÅ‚Ã³w (np. "i", "w", "na").

```
sample=1e-3 (0.001): SÅ‚owa czÄ™stsze niÅ¼ 0.1% sÄ… czÄ™Å›ciej pomijane
sample=1e-4 (0.0001): Bardziej agresywne pomijanie
sample=1e-5 (0.00001): Bardzo agresywne pomijanie
```

**Dlaczego to waÅ¼ne?**
- SÅ‚owa typu "i", "w", "na" wystÄ™pujÄ… bardzo czÄ™sto
- Nie niosÄ… duÅ¼o informacji semantycznej
- Pomijanie ich przyspiesza trening i poprawia jakoÅ›Ä‡

**Praktyczna zasada:** `sample=1e-3` to dobry default.

---

## ğŸ”¬ Obserwacje z eksperymentÃ³w

### Obserwacja 1: Tokenizer ma najwiÄ™kszy wpÅ‚yw

| Tokenizer | krÃ³l-ksiÄ…Å¼Ä™ | Uwagi |
|-----------|-------------|-------|
| **all-corpora** (wÅ‚asny) | **0.7091** âœ… | Najlepszy dla polskiego |
| bielik-v3 | 0.6228 | Dobry, uniwersalny |
| bielik-v1 | ~0.60 | Oparty na Mistral (angielski) |

**Dlaczego?**
- WÅ‚asny tokenizer byÅ‚ trenowany na **tym samym korpusie** co model CBOW
- Bielik byÅ‚ trenowany na **innych danych** (internet, Wikipedia, etc.)
- Dopasowanie tokenizera do korpusu = lepsze wyniki

---

### Obserwacja 2: WielkoÅ›Ä‡ korpusu ma znaczenie

| Korpus | Liczba zdaÅ„ | krÃ³l-ksiÄ…Å¼Ä™ |
|--------|-------------|-------------|
| PAN_TADEUSZ | ~3,000 | ~0.45 |
| WOLNELEKTURY | ~100,000 | ~0.60 |
| ALL | ~140,000 | **0.71** |

**Dlaczego?**
- WiÄ™cej danych = wiÄ™cej kontekstÃ³w dla kaÅ¼dego sÅ‚owa
- Model "widzi" sÅ‚owo w rÃ³Å¼nych sytuacjach
- Lepiej uczy siÄ™ relacji semantycznych

---

### Obserwacja 3: Punkt nasycenia dla epochs

```
epochs=10:  krÃ³l-ksiÄ…Å¼Ä™ = 0.62
epochs=20:  krÃ³l-ksiÄ…Å¼Ä™ = 0.68
epochs=30:  krÃ³l-ksiÄ…Å¼Ä™ = 0.71
epochs=50:  krÃ³l-ksiÄ…Å¼Ä™ = 0.72  â† niewielka poprawa
epochs=100: krÃ³l-ksiÄ…Å¼Ä™ = 0.72  â† brak poprawy
```

**Wniosek:** Po ~30-50 epokach wyniki siÄ™ stabilizujÄ…. Dalsze trenowanie to strata czasu.

---

### Obserwacja 4: Trade-off vector_size vs korpus

| Korpus | vector_size=50 | vector_size=100 | vector_size=200 |
|--------|----------------|-----------------|-----------------|
| MaÅ‚y (3k zdaÅ„) | 0.45 | 0.42 | 0.38 âš ï¸ |
| Åšredni (100k) | 0.58 | 0.65 | 0.68 |
| DuÅ¼y (140k) | 0.62 | **0.71** | 0.72 |

**Wniosek:** 
- Dla maÅ‚ych korpusÃ³w: mniejszy `vector_size` jest lepszy (mniej parametrÃ³w do nauczenia)
- Dla duÅ¼ych korpusÃ³w: wiÄ™kszy `vector_size` moÅ¼e pomÃ³c

---

### Obserwacja 5: Prefiks `â–` w Bielik

Tokenizery SentencePiece (Bielik, LLaMA, Mistral) dodajÄ… `â–` przed sÅ‚owami:

```
Bielik tokenizuje "krÃ³l" jako:  ["â–krÃ³l"]
WÅ‚asny BPE tokenizuje jako:     ["krÃ³l"]
```

**Co to oznacza?**
- `â–` (Unicode U+2581) oznacza poczÄ…tek sÅ‚owa
- W wynikach widzisz `â–dziewczyna`, `â–matka` - to normalne
- Tokeny bez `â–` to czÄ™Å›ci sÅ‚Ã³w: `ta` w `szlach|ta`

---

### Obserwacja 6: SÅ‚owa wielotokenowe wymagajÄ… uÅ›redniania

```python
# SÅ‚owo "szlachta" jest tokenizowane jako:
"szlachta" â†’ ["szlach", "ta"]

# Musimy uÅ›redniÄ‡ wektory:
vec("szlachta") = (vec("szlach") + vec("ta")) / 2
```

**Problem:** UÅ›rednianie moÅ¼e rozmywaÄ‡ znaczenie.

**RozwiÄ…zanie:** UÅ¼ywaj tokenizera, ktÃ³ry tworzy dÅ‚uÅ¼sze tokeny (wÅ‚asny `all-corpora`).

---

## ğŸ¯ Optymalne ustawienia (podsumowanie)

Na podstawie eksperymentÃ³w, najlepsze wyniki dla polskiego tekstu:

```bash
python cbow-train.py \
    --corpus ALL \
    --tokenizer all-corpora \
    --vector-size 100 \
    --window 5 \
    --epochs 30 \
    --min-count 2 \
    --sample 0.001
```

| Parametr | WartoÅ›Ä‡ | Uzasadnienie |
|----------|---------|--------------|
| `corpus` | ALL | Maksymalna iloÅ›Ä‡ danych |
| `tokenizer` | all-corpora | Dopasowany do korpusu |
| `vector_size` | 100 | Dobry kompromis dla ~140k zdaÅ„ |
| `window` | 5 | Standardowe, Å‚apie relacje semantyczne |
| `epochs` | 30 | Punkt nasycenia |
| `min_count` | 2 | Filtruje literÃ³wki |
| `sample` | 1e-3 | Redukuje wpÅ‚yw "i", "w", "na" |

---

## ğŸ“ OmÃ³wienie plikÃ³w projektu

### 1. `cbow-train.py` - Trening modelu

Skrypt do **trenowania modelu CBOW** z rÃ³Å¼nymi konfiguracjami.

```python
# ARGUMENTY CLI
parser.add_argument("--corpus", ...)      # WybÃ³r korpusu: ALL, WOLNELEKTURY, NKJP
parser.add_argument("--tokenizer", ...)   # WybÃ³r tokenizera: bielik-v1, bielik-v3, all-corpora
parser.add_argument("--vector-size", ...) # Wymiar wektora (default: 100)
parser.add_argument("--window", ...)      # Okno kontekstowe (default: 5)
parser.add_argument("--epochs", ...)      # Liczba epok (default: 20)
```

**PrzykÅ‚adowe wywoÅ‚ania:**
```bash
# Podstawowe
python cbow-train.py --corpus ALL --tokenizer bielik-v3

# Z parametrami
python cbow-train.py --corpus ALL --tokenizer all-corpora --epochs 50 --vector-size 200
```

**Co robi skrypt:**
1. Åaduje tokenizer (BPE)
2. Wczytuje korpus tekstowy
3. Tokenizuje zdania
4. Trenuje model Word2Vec (CBOW)
5. Zapisuje model i pliki pomocnicze

**Pliki wyjÅ›ciowe:**
```
models/
â”œâ”€â”€ cbow_all_bielik-v3.model       # Model gensim
â”œâ”€â”€ cbow_all_bielik-v3_tensor.npy  # Macierz embeddingÃ³w (NumPy)
â”œâ”€â”€ cbow_all_bielik-v3_token_map.json  # Mapowanie token â†’ indeks
â””â”€â”€ cbow_all_bielik-v3_config.json # Konfiguracja modelu
```

---

### 2. `cbow-infer.py` - Wnioskowanie

Skrypt do **testowania wytrenowanego modelu** - szukanie podobnych sÅ‚Ã³w.

```python
# ARGUMENTY CLI
parser.add_argument("--model", ...)       # ÅšcieÅ¼ka do modelu .model
parser.add_argument("--tokenizer", ...)   # Tokenizer uÅ¼yty podczas treningu
parser.add_argument("--words", ...)       # Lista sÅ‚Ã³w do testowania
parser.add_argument("--analogy", ...)     # Para sÅ‚Ã³w do analogii
parser.add_argument("--interactive", ...) # Tryb interaktywny
```

**PrzykÅ‚adowe wywoÅ‚ania:**
```bash
# Podstawowe testy
python cbow-infer.py --model models/cbow_all_bielik-v3.model --tokenizer bielik-v3

# Konkretne sÅ‚owa
python cbow-infer.py --model models/cbow_all_bielik-v3.model --tokenizer bielik-v3 --words krÃ³l,ksiÄ…Å¼Ä™,wojsko

# Analogia (dziecko + kobieta = ?)
python cbow-infer.py --model models/cbow_all_bielik-v3.model --tokenizer bielik-v3 --analogy dziecko,kobieta

# Tryb interaktywny
python cbow-infer.py --model models/cbow_all_bielik-v3.model --tokenizer bielik-v3 --interactive
```

**Kluczowa funkcja - `get_word_vector()`:**
```python
def get_word_vector(word: str, tokenizer: Tokenizer, model: Word2Vec):
    """
    Oblicza wektor dla sÅ‚owa poprzez uÅ›rednienie wektorÃ³w jego tokenÃ³w.
    
    PrzykÅ‚ad dla sÅ‚owa "szlachta":
    1. Tokenizacja: "szlachta" â†’ ["szla", "chta"]
    2. Pobierz wektory: vec("szla"), vec("chta")
    3. UÅ›rednij: (vec("szla") + vec("chta")) / 2
    """
    encoding = tokenizer.encode(" " + word + " ")
    word_tokens = [t.strip() for t in encoding.tokens if t.strip()]
    
    valid_vectors = []
    for token in word_tokens:
        if token in model.wv:
            valid_vectors.append(model.wv[token])
    
    if not valid_vectors:
        return None
    
    return np.mean(valid_vectors, axis=0)  # UÅ›rednienie wektorÃ³w
```

---

### 3. `cbow-compare.py` - PorÃ³wnanie konfiguracji

Skrypt do **automatycznego porÃ³wnania** rÃ³Å¼nych kombinacji parametrÃ³w.

```bash
# Szybki test (2 konfiguracje)
python cbow-compare.py --quick

# PorÃ³wnanie tylko tokenizerÃ³w
python cbow-compare.py --tokenizers-only

# PeÅ‚ne porÃ³wnanie (wiele kombinacji)
python cbow-compare.py

# Zapisz wyniki
python cbow-compare.py --save-results results.json
```

**Co testuje:**
- RÃ³Å¼ne tokenizery: bielik-v1, bielik-v3, all-corpora
- RÃ³Å¼ne `vector_size`: 50, 100, 200
- RÃ³Å¼ne `window`: 3, 5, 8
- RÃ³Å¼ne `epochs`: 10, 30, 50

**Metryka oceny:**
```python
REFERENCE_PAIRS = [
    ("krÃ³l", "ksiÄ…Å¼Ä™"),      # Cel: ~0.7+
    ("kobieta", "dziecko"),  # Cel: ~0.6+
    ("wojsko", "armia"),     # Cel: ~0.6+
    ("szlachta", "rycerz"),  # Cel: ~0.5+
]
```

---

### 4. `corpora.py` - ZarzÄ…dzanie korpusami

Ten sam plik co w zadaniu 3 - definiuje dostÄ™pne korpusy.

```python
CORPORA_FILES = {
    "NKJP": [...],           # Narodowy Korpus JÄ™zyka Polskiego
    "WOLNELEKTURY": [...],   # Wolne Lektury
    "PAN_TADEUSZ": [...],    # Tylko Pan Tadeusz
    "ALL": [...],            # Wszystkie poÅ‚Ä…czone
}
```

---

## ğŸ SkÅ‚adnia Pythona - kluczowe elementy

### 1. NumPy - operacje na wektorach

```python
import numpy as np

# Tworzenie wektora
vec = np.array([1.0, 2.0, 3.0])

# UÅ›rednianie listy wektorÃ³w
vectors = [vec1, vec2, vec3]
mean_vector = np.mean(vectors, axis=0)  # axis=0 = uÅ›rednianie kolumnami

# Norma wektora (dÅ‚ugoÅ›Ä‡)
norm = np.linalg.norm(vec)  # sqrt(1Â² + 2Â² + 3Â²)

# Iloczyn skalarny
dot_product = np.dot(vec1, vec2)

# PodobieÅ„stwo kosinusowe
cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
```

### 2. Gensim - Word2Vec

```python
from gensim.models import Word2Vec

# Trening modelu
model = Word2Vec(
    sentences=tokenized_sentences,  # Lista list tokenÃ³w
    vector_size=100,                # Wymiar wektora
    window=5,                       # Okno kontekstowe
    min_count=2,                    # Min. wystÄ…pieÅ„
    workers=4,                      # Liczba wÄ…tkÃ³w
    sg=0,                           # 0=CBOW, 1=Skip-gram
    epochs=20,                      # Liczba epok
)

# DostÄ™p do wektorÃ³w
vector = model.wv["krÃ³l"]          # Wektor dla tokenu
similar = model.wv.most_similar("krÃ³l", topn=10)  # Podobne tokeny

# Sprawdzenie czy token jest w sÅ‚owniku
if "krÃ³l" in model.wv:
    print("Token istnieje!")

# Zapisywanie i Å‚adowanie
model.save("model.model")
model = Word2Vec.load("model.model")
```

### 3. Rich - kolorowy output w terminalu

```python
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

console = Console()

# Kolorowy tekst
console.print("[bold green]Sukces![/bold green]")
console.print("[red]BÅ‚Ä…d[/red]")

# Tabela
table = Table(title="Wyniki")
table.add_column("SÅ‚owo", style="cyan")
table.add_column("PodobieÅ„stwo", justify="right")
table.add_row("krÃ³l", "0.72")
console.print(table)

# Panel (ramka)
panel = Panel("ZawartoÅ›Ä‡", title="TytuÅ‚")
console.print(panel)
```

### 4. Argparse - argumenty CLI

```python
import argparse

parser = argparse.ArgumentParser(description="Opis programu")

# Argument wymagany
parser.add_argument("--model", type=str, required=True, help="ÅšcieÅ¼ka do modelu")

# Argument z wartoÅ›ciÄ… domyÅ›lnÄ…
parser.add_argument("--epochs", type=int, default=20, help="Liczba epok")

# Argument z dozwolonymi wartoÅ›ciami
parser.add_argument("--tokenizer", choices=["bielik-v1", "bielik-v3"], default="bielik-v3")

# Argument typu flag (True/False)
parser.add_argument("--interactive", action="store_true", help="Tryb interaktywny")

# Parsowanie
args = parser.parse_args()
print(args.model, args.epochs)
```

---

## ğŸš€ Jak uruchomiÄ‡

### Krok 1: Przygotowanie Å›rodowiska

```bash
cd M1/embedding

# UtwÃ³rz Å›rodowisko wirtualne z Python 3.11 (gensim nie dziaÅ‚a na Python 3.14)
# JeÅ›li masz pyenv:
~/.pyenv/versions/3.11.*/bin/python -m venv venv

# Lub jeÅ›li python3.11 jest w PATH:
python3.11 -m venv venv

# Aktywuj Å›rodowisko
source venv/bin/activate

# SprawdÅº wersjÄ™ (powinno byÄ‡ 3.11.x)
python --version

# Zainstaluj pakiety
pip install -r requirements.txt
```

**Uwaga:** Gensim wymaga Python 3.10-3.12. JeÅ›li masz tylko Python 3.14, zainstaluj starszÄ… wersjÄ™ przez `pyenv install 3.11`.

### Krok 2: Upewnij siÄ™, Å¼e tokenizery sÄ… dostÄ™pne

Tokenizery z Zadania 3 powinny byÄ‡ w `../tokenizer/tokenizers/`:
```bash
ls ../tokenizer/tokenizers/
# Oczekiwane pliki:
# bielik-v1-tokenizer.json
# bielik-v3-tokenizer.json
# tokenizer-all-corpora.json
# ...
```

### Krok 3: Trenuj model

```bash
# Podstawowy trening
python cbow-train.py --corpus ALL --tokenizer bielik-v3 --epochs 30

# Z wÅ‚asnym tokenizerem
python cbow-train.py --corpus ALL --tokenizer all-corpora --epochs 50 --vector-size 150
```

### Krok 4: Testuj model

```bash
# Podstawowe testy
python cbow-infer.py --model models/cbow_all_bielik-v3.model --tokenizer bielik-v3 --all-tests

# Interaktywny
python cbow-infer.py --model models/cbow_all_bielik-v3.model --tokenizer bielik-v3 --interactive
```

### Krok 5: PorÃ³wnaj konfiguracje

```bash
python cbow-compare.py --tokenizers-only
```

---

## ğŸ“ˆ Eksperymenty i wnioski

### Cel zadania
ZnaleÅºÄ‡ takie ustawienia, aby sÅ‚owa pokrewne miaÅ‚y wysokie podobieÅ„stwo:
- **krÃ³l-ksiÄ…Å¼Ä™**: cel > 0.7
- **dziecko+kobieta â†’ dziewczyna**: cel > 0.6

### Spodziewane wyniki

| Tokenizer | krÃ³l-ksiÄ…Å¼Ä™ | Uwagi |
|-----------|-------------|-------|
| WÅ‚asny (all-corpora) | ~0.72 | Najlepszy dla polskiego |
| bielik-v3 | ~0.70 | Dobry, nowszy |
| bielik-v1 | ~0.65 | Oparty na Mistral |

### RÃ³Å¼nice miÄ™dzy tokenizerami

**Bielik (SentencePiece):**
```
"krÃ³l" â†’ ["â–krÃ³l"]
"szlachta" â†’ ["â–szlach", "ta"]
"dziewczyna" â†’ ["â–dziewczyna"]
```
- UÅ¼ywa prefiksu `â–` (Unicode: U+2581) przed tokenami rozpoczynajÄ…cymi sÅ‚owo
- Jest to standard dla modeli LLaMA/Mistral/Bielik
- W wynikach widzisz `â–ona`, `â–matka`, `â–dziewczyna`

**WÅ‚asny BPE (tokenizers library):**
```
"krÃ³l" â†’ ["krÃ³l"]
"szlachta" â†’ ["szlachta"]  # lub ["szlach", "ta"] zaleÅ¼nie od korpusu
"dziewczyna" â†’ ["dziewczyna"]
```
- Nie uÅ¼ywa prefiksu `â–`
- Tokeny wyglÄ…dajÄ… "normalnie"
- Prostsze do interpretacji wynikÃ³w

### Kluczowe obserwacje

1. **Tokenizer ma znaczenie** - wÅ‚asny tokenizer (trenowany na polskim korpusie) daje lepsze wyniki dla polskiego tekstu.

2. **Korpus ma znaczenie** - wiÄ™kszy korpus (ALL) daje lepsze wyniki niÅ¼ maÅ‚y (PAN_TADEUSZ).

3. **Parametry:**
   - `vector_size=100-150` - dobry kompromis
   - `window=5-8` - wystarczajÄ…cy kontekst
   - `epochs=30-50` - po ~50 wyniki siÄ™ stabilizujÄ…

4. **SÅ‚owa wielotokenowe** - sÅ‚owa dzielone na wiele tokenÃ³w (np. "szlachta" â†’ ["szla", "chta"]) wymagajÄ… uÅ›redniania wektorÃ³w.

5. **Prefiks `â–` w Bielik** - tokenizery typu SentencePiece (Bielik, LLaMA, Mistral) dodajÄ… znak `â–` przed tokenami rozpoczynajÄ…cymi nowe sÅ‚owo. To normalne zachowanie:
   - `â–krÃ³l` = token "krÃ³l" na poczÄ…tku sÅ‚owa
   - `â–dziewczyna` = peÅ‚ne sÅ‚owo "dziewczyna"
   - `ta` (bez `â–`) = czÄ™Å›Ä‡ sÅ‚owa (np. w "szlach**ta**")
   
   W wynikach podobieÅ„stwa widzisz wiÄ™c `â–ona`, `â–matka`, `â–dziewczyna` - to sÄ… peÅ‚ne sÅ‚owa z prefiksem.

### Dalsze eksperymenty

1. **Skip-gram** - zmieÅ„ `--sg 1` i porÃ³wnaj wyniki. Skip-gram czÄ™sto lepszy dla rzadkich sÅ‚Ã³w.

2. **WiÄ™kszy korpus** - dodaj wiÄ™cej tekstÃ³w do korpusu treningowego.

3. **Preprocessing** - usuÅ„ znaki specjalne, zamieÅ„ na maÅ‚e litery przed tokenizacjÄ….

---

## ğŸ”— Przydatne zasoby

- [Gensim Word2Vec Tutorial](https://radimrehurek.com/gensim/models/word2vec.html)
- [Word2Vec Paper](https://arxiv.org/abs/1301.3781)
- [Ilustrowany Word2Vec](https://jalammar.github.io/illustrated-word2vec/)
