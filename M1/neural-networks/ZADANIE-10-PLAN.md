# Zadanie 10 â€” Dotrenowanie sieci circle-in-square do 100% accuracy

## Opis problemu

SieÄ‡ ma rozwiÄ…zaÄ‡ problem **klasyfikacji binarnej**: czy punkt $(x, y)$ leÅ¼y wewnÄ…trz okrÄ™gu o promieniu $0.5$ (etykieta **1**) czy poza nim (etykieta **0**), gdy punkt znajduje siÄ™ w kwadracie $[-1, 1] \times [-1, 1]$.

Kod w `circle-in-square-network.py` jest praktycznie gotowy, ale ma **celowo zaniÅ¼one** 4 parametry oznaczone ğŸ”¥. Naszym celem jest je poprawiÄ‡, aby sieÄ‡ osiÄ…gnÄ™Å‚a **100% accuracy przy maÅ‚ym rozmiarze**.

---

## SÅ‚ownik pojÄ™Ä‡

| PojÄ™cie | WyjaÅ›nienie |
|---|---|
| **SieÄ‡ neuronowa (Neural Network)** | Program inspirowany dziaÅ‚aniem mÃ³zgu â€” dane wejÅ›ciowe przechodzÄ… przez warstwy "neuronÃ³w" (mnoÅ¼enie przez wagi + dodanie biasu), a na koÅ„cu wychodzi wynik. SieÄ‡ "uczy siÄ™" dobierajÄ…c wagi tak, by wynik byÅ‚ poprawny. |
| **Klasyfikacja binarna** | Zadanie "odpowiedz 0 albo 1". Tutaj: punkt leÅ¼y wewnÄ…trz okrÄ™gu (1) lub nie (0). |
| **Warstwa (Layer)** | Grupa neuronÃ³w na jednym "piÄ™trze" sieci. `nn.Linear(2, 8)` = warstwa z 2 wejÅ›ciami i 8 neuronami. |
| **Warstwa ukryta (Hidden Layer)** | Warstwa miÄ™dzy wejÅ›ciem a wyjÅ›ciem â€” tutaj sieÄ‡ "myÅ›li". Im wiÄ™cej neuronÃ³w, tym bardziej zÅ‚oÅ¼one wzorce moÅ¼e wykryÄ‡. |
| **Neuron** | Pojedynczy "wÄ™zeÅ‚" â€” liczy sumÄ™ waÅ¼onÄ… wejÅ›Ä‡, dodaje bias, przepuszcza przez funkcjÄ™ aktywacji. |
| **Funkcja aktywacji (ReLU)** | Funkcja nieliniowa: $ReLU(x) = \max(0, x)$. Bez niej sieÄ‡ byÅ‚aby zwykÅ‚ym mnoÅ¼eniem macierzy â€” nie umiaÅ‚aby uczyÄ‡ siÄ™ krzywych/okrÄ™gÃ³w. |
| **Sigmoid** | Funkcja Å›ciskajÄ…ca wynik do zakresu $(0, 1)$: $\sigma(x) = \frac{1}{1+e^{-x}}$. Interpretujemy wynik jako prawdopodobieÅ„stwo. |
| **Logit** | Surowy wynik sieci PRZED zastosowaniem Sigmoida. `BCEWithLogitsLoss` Å‚Ä…czy Sigmoid + loss w jednym kroku (numerycznie stabilniej). |
| **Loss (Strata)** | Miara "jak bardzo sieÄ‡ siÄ™ myli". Im mniejszy loss â†’ tym lepsza sieÄ‡. `BCEWithLogitsLoss` = Binary Cross-Entropy. |
| **Epoka (Epoch)** | Jedno przejÅ›cie przez CAÅY zbiÃ³r danych treningowych. 2000 epok = sieÄ‡ zobaczyÅ‚a dane 2000 razy. |
| **Learning Rate** | Jak duÅ¼y krok robi optymalizator przy aktualizacji wag. Za maÅ‚y â†’ sieÄ‡ uczy siÄ™ wieki. Za duÅ¼y â†’ "przeskakuje" optimum. |
| **Optymalizator Adam** | Zaawansowany algorytm aktualizacji wag â€” adaptuje learning rate per parametr. Lepszy od prostego SGD dla wiÄ™kszoÅ›ci zastosowaÅ„. |
| **Backward Pass (Backpropagation)** | Algorytm obliczajÄ…cy gradienty (pochodne) â€” mÃ³wi sieci "w ktÃ³rÄ… stronÄ™ i o ile zmieniÄ‡ kaÅ¼dÄ… wagÄ™, by zmniejszyÄ‡ loss". |
| **Gradient** | Kierunek i wielkoÅ›Ä‡ zmiany wagi â€” wskazuje jak szybko loss roÅ›nie/maleje przy zmianie danej wagi. |
| **PrÃ³bki (Samples)** | Punkty treningowe. 10 punktÃ³w to ZA MAÅO, by sieÄ‡ "zobaczyÅ‚a" okrÄ…g. Potrzeba setek/tysiÄ™cy. |
| **TensorBoard** | NarzÄ™dzie wizualizacyjne â€” wykresy loss, histogramy wag/gradientÃ³w. Pozwala zobaczyÄ‡ CZY i JAK sieÄ‡ siÄ™ uczy. |
| **Accuracy** | Procent poprawnych odpowiedzi: $\frac{\text{poprawne predykcje}}{\text{wszystkie prÃ³bki}} \times 100\%$ |

---

## Co jest Åºle w obecnym kodzie i dlaczego

W `circle-in-square-network.py` sÄ… **4 celowo zaniÅ¼one wartoÅ›ci**:

### 1. Struktura sieci (linie 23â€“24)

```python
# OBECNIE:
self.fc1 = nn.Linear(2, 2)
self.fc2 = nn.Linear(2, 1)
```

SieÄ‡ ma tylko **2 neurony w jednej warstwie ukrytej**. Aby odwzorowaÄ‡ okrÄ…g (granicÄ™ nieliniowÄ…), potrzeba wiÄ™cej neuronÃ³w. WyobraÅº sobie, Å¼e kaÅ¼dy neuron z ReLU to jedna linia prosta â€” aby "narysowaÄ‡" okrÄ…g, potrzebujesz wielu odcinkÃ³w prostych.

### 2. Liczba prÃ³bek (linia 49)

```python
# OBECNIE:
NUM_SAMPLES = 10
```

Z 10 punktÃ³w sieÄ‡ nie jest w stanie "zobaczyÄ‡" ksztaÅ‚tu okrÄ™gu. To jakbyÅ› miaÅ‚ 10 punktÃ³w na mapie i prÃ³bowaÅ‚ odgadnÄ…Ä‡ ksztaÅ‚t Polski.

### 3. Learning rate (linia 56)

```python
# OBECNIE:
LEARNING_RATE = 0.00001
```

Absurdalnie maÅ‚y. Przy Adamie typowa wartoÅ›Ä‡ to `0.001`â€“`0.01`. Z tak maÅ‚ym LR sieÄ‡ potrzebowaÅ‚aby milionÃ³w epok.

### 4. Liczba epok (linia 57)

```python
# OBECNIE:
EPOCHS = 500
```

Za maÅ‚o, biorÄ…c pod uwagÄ™ resztÄ™ problemÃ³w. Przy poprawionych parametrach 1000â€“3000 powinno wystarczyÄ‡.

---

## Kroki do wykonania

### Krok 0: Przygotowanie Å›rodowiska

```bash
cd M1/neural-networks
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

> **Uwaga**: TensorBoard zainstaluje siÄ™ automatycznie z `requirements.txt` â€” nie wymaga osobnej instalacji.

### Krok 1: PowiÄ™ksz strukturÄ™ sieci

ZmieÅ„ klasÄ™ `CircleInSquareNet` (linie 21â€“29):

```python
# BYÅO:
class CircleInSquareNet(nn.Module):
    def __init__(self):
        super(CircleInSquareNet, self).__init__()
        self.fc1 = nn.Linear(2, 2)
        self.fc2 = nn.Linear(2, 1)

    def forward(self, x):
        x = nn.ReLU()(self.fc1(x))
        x = self.fc2(x)
        return x
```

```python
# MA BYÄ†:
class CircleInSquareNet(nn.Module):
    def __init__(self):
        super(CircleInSquareNet, self).__init__()
        self.fc1 = nn.Linear(2, 16)   # 2 wejÅ›cia â†’ 16 neuronÃ³w
        self.fc2 = nn.Linear(16, 8)   # 16 â†’ 8 neuronÃ³w
        self.fc3 = nn.Linear(8, 1)    # 8 â†’ 1 wyjÅ›cie

    def forward(self, x):
        x = nn.ReLU()(self.fc1(x))
        x = nn.ReLU()(self.fc2(x))
        x = self.fc3(x)
        return x
```

**Dlaczego 16 â†’ 8 â†’ 1?**
- Granica decyzyjna to okrÄ…g â€” krzywa nieliniowa
- ReLU tworzy odcinki proste, wiÄ™c potrzebujesz wystarczajÄ…co wielu neuronÃ³w, by z odcinkÃ³w "zÅ‚oÅ¼yÄ‡" okrÄ…g
- 16 neuronÃ³w w pierwszej warstwie to wystarczajÄ…ca iloÅ›Ä‡ do aproksymacji
- Druga warstwa (8) pomaga lepiej kombinowaÄ‡ wyuczone cechy
- Cel zadania mÃ³wi "przy maÅ‚ym rozmiarze" â€” 16+8 to dobry kompromis

**Alternatywy**:
| Struktura | Komentarz |
|---|---|
| 2 â†’ 8 â†’ 1 | MoÅ¼e zadziaÅ‚aÄ‡, ale wymaga wiÄ™cej epok |
| 2 â†’ 10 â†’ 1 | RozsÄ…dna opcja, minimalnie mniejsza |
| 2 â†’ 32 â†’ 16 â†’ 1 | Overkill â€” za duÅ¼a sieÄ‡ jak na tak proste zadanie |

### Krok 2: ZwiÄ™ksz liczbÄ™ prÃ³bek

```python
# BYÅO:
NUM_SAMPLES = 10

# MA BYÄ†:
NUM_SAMPLES = 1000
```

**Dlaczego 1000?** Aby sieÄ‡ "zobaczyÅ‚a" okrÄ…g â€” potrzebuje punktÃ³w gÄ™sto rozmieszczonych w kwadracie $[-1, 1] \times [-1, 1]$. 1000 punktÃ³w daje wystarczajÄ…cÄ… gÄ™stoÅ›Ä‡.

### Krok 3: ZwiÄ™ksz learning rate

```python
# BYÅO:
LEARNING_RATE = 0.00001

# MA BYÄ†:
LEARNING_RATE = 0.01
```

**Dlaczego 0.01?** Optymalizator Adam adaptuje LR sam, ale startowa wartoÅ›Ä‡ powinna byÄ‡ rozsÄ…dna. Obecna wartoÅ›Ä‡ `0.00001` jest **1000Ã— za maÅ‚a**.

### Krok 4: ZwiÄ™ksz liczbÄ™ epok

```python
# BYÅO:
EPOCHS = 500

# MA BYÄ†:
EPOCHS = 2000
```

### Krok 5: Dostosuj logowanie TensorBoard

Po dodaniu warstwy `fc3` â€” dodaj logowanie gradientÃ³w i wag dla nowej warstwy.

### Krok 6: Uruchom i zweryfikuj

```bash
python circle-in-square-network.py
```

**Oczekiwany wynik**: `DokÅ‚adnoÅ›Ä‡ na zbiorze treningowym: 100.00%` i `Good job! ğŸ‰`

### Krok 7 (opcjonalny): Wizualizacja w TensorBoard

```bash
tensorboard --logdir=runs
```

OtwÃ³rz http://localhost:6006/

---

## Podsumowanie zmian

| Parametr | ByÅ‚o | Ma byÄ‡ |
|---|---|---|
| Struktura sieci | 2 â†’ 2 â†’ 1 | 2 â†’ 16 â†’ 8 â†’ 1 |
| NUM_SAMPLES | 10 | 1000 |
| LEARNING_RATE | 0.00001 | 0.01 |
| EPOCHS | 500 | 2000 |

---

## Pomocne narzÄ™dzia do budowania intuicji

### TensorFlow Playground
https://playground.tensorflow.org

- Ustaw problem na **"Circle"** (ikona okrÄ™gu)
- Dodawaj/usuwaj neurony i warstwy
- Obserwuj jak sieÄ‡ "rysuje" granicÄ™ decyzyjnÄ… w czasie rzeczywistym

### TensorBoard
- Zainstaluje siÄ™ razem z `requirements.txt`
- Uruchom: `tensorboard --logdir=runs`
- OtwÃ³rz: http://localhost:6006/
- Sekcje: SCALARS (loss), HISTOGRAMS (gradienty i wagi)

---

## TensorBoard â€” czym jest, jak dziaÅ‚a, jak czytaÄ‡ wykresy

### Czym jest TensorBoard?

TensorBoard to **narzÄ™dzie wizualizacyjne** stworzone przez Google do monitorowania procesu treningu sieci neuronowych. ChoÄ‡ powstaÅ‚o jako czÄ™Å›Ä‡ TensorFlow, dziaÅ‚a doskonale z PyTorch dziÄ™ki moduÅ‚owi `torch.utils.tensorboard.SummaryWriter`.

MoÅ¼na o nim myÅ›leÄ‡ jak o **"dashboardzie" dla treningu** â€” zamiast wpatrywaÄ‡ siÄ™ w liczby lecÄ…ce przez terminal, dostajesz interaktywne wykresy, ktÃ³re natychmiast pokazujÄ… czy sieÄ‡ uczy siÄ™ dobrze, Åºle, czy wcale.

### Jak dziaÅ‚a?

1. **Zapis danych** â€” w kodzie treningowym `SummaryWriter` zapisuje metryki (loss, wagi, gradienty) do plikÃ³w logÃ³w w folderze `runs/`:
   ```python
   writer = SummaryWriter('runs/circle_in_square')
   writer.add_scalar('Loss', loss.item(), epoch)          # wartoÅ›Ä‡ skalarna
   writer.add_histogram('Weights/FC1', model.fc1.weight.data, epoch)  # rozkÅ‚ad
   ```
2. **Serwer HTTP** â€” komenda `tensorboard --logdir=runs` uruchamia lokalny serwer, ktÃ³ry czyta logi i generuje interaktywne wykresy.
3. **PrzeglÄ…darka** â€” otwierasz http://localhost:6006/ i widzisz wyniki.

Dane zapisywane sÄ… **przyrostowo** â€” moÅ¼esz uruchomiÄ‡ TensorBoard w trakcie treningu i obserwowaÄ‡ postÄ™py na Å¼ywo (odÅ›wieÅ¼ stronÄ™ lub kliknij ikonÄ™ odÅ›wieÅ¼ania).

### ZakÅ‚adki TensorBoard

#### 1. SCALARS â€” "Czy sieÄ‡ siÄ™ uczy?"

To najwaÅ¼niejsza zakÅ‚adka. Pokazuje **wykres wartoÅ›ci skalarnych w czasie** (oÅ› X = epoki, oÅ› Y = wartoÅ›Ä‡).

**Wykres Loss (Strata)**:
- **Dobry trening**: loss **monotonnie spada** od duÅ¼ej wartoÅ›ci do bliskiej zeru. W naszym przypadku: od ~0.17 do ~0.004.
- **ZÅ‚y trening (za maÅ‚y LR)**: loss praktycznie siÄ™ nie zmienia â€” linia jest pÅ‚aska.
- **ZÅ‚y trening (za duÅ¼y LR)**: loss skacze w gÃ³rÄ™ i w dÃ³Å‚ chaotycznie (oscylacje).
- **Overfitting**: loss treningowy spada, ale loss walidacyjny (gdyby byÅ‚) roÅ›nie â€” sieÄ‡ "uczy siÄ™ na pamiÄ™Ä‡".

**Jak czytaÄ‡**: JeÅ›li krzywa loss:
- Spada szybko na poczÄ…tku, potem zwalnia â†’ **OK** (typowe)
- Jest pÅ‚aska â†’ learning rate za maÅ‚y lub sieÄ‡ za maÅ‚a
- Skacze w gÃ³rÄ™/dÃ³Å‚ â†’ learning rate za duÅ¼y
- Spada do 0 â†’ ideaÅ‚

**Suwak "Smoothing"** (po prawej, np. 0.6): wygÅ‚adza krzywÄ… â€” przydatny gdy loss oscyluje. WartoÅ›Ä‡ 0 = surowe dane, 1 = maksymalne wygÅ‚adzenie.

#### 2. HISTOGRAMS â€” "Co dzieje siÄ™ wewnÄ…trz sieci?"

Pokazuje **rozkÅ‚ady wartoÅ›ci w czasie** w formie 3D "gÃ³r". OÅ› X = wartoÅ›ci (np. wagi), oÅ› Y = epoki (czas), oÅ› Z (gÅ‚Ä™bokoÅ›Ä‡) = gÄ™stoÅ›Ä‡.

**Histogramy gradientÃ³w (Gradients/Layer_FCx_Weights)**:
- PokazujÄ… rozkÅ‚ad gradientÃ³w (pochodnych) dla wag kaÅ¼dej warstwy.
- **Zdrowe gradienty**: rozkÅ‚ad skupiony wokÃ³Å‚ zera, ale NIE dokÅ‚adnie na zerze â€” sieÄ‡ aktywnie siÄ™ uczy.
- **Vanishing gradients** (zanikajÄ…ce gradienty): gradienty skupione na dokÅ‚adnie 0 â†’ sieÄ‡ przestaje siÄ™ uczyÄ‡ (problem gÅ‚Ä™bokich sieci).
- **Exploding gradients** (eksplodujÄ…ce gradienty): wartoÅ›ci gradientÃ³w rosnÄ… do ogromnych liczb â†’ trening staje siÄ™ niestabilny.
- **Stabilizacja w czasie**: w miarÄ™ jak sieÄ‡ siÄ™ uczy, gradienty powinny maleÄ‡ (bo loss maleje, wiÄ™c korekty sÄ… coraz mniejsze).

**Histogramy wag (Weights/Layer_FCx_Weights)**:
- PokazujÄ… jak zmieniajÄ… siÄ™ wagi (parametry) sieci w czasie treningu.
- **Na poczÄ…tku**: wagi sÄ… losowe (inicjalizacja), rozkÅ‚ad jest maÅ‚y i chaotyczny.
- **W trakcie treningu**: wagi "rozchodzÄ… siÄ™" â€” sieÄ‡ ksztaÅ‚tuje swojÄ… wewnÄ™trznÄ… reprezentacjÄ™.
- **Na koÅ„cu treningu**: rozkÅ‚ad powinien siÄ™ ustabilizowaÄ‡ (nie zmienia siÄ™ drastycznie).
- JeÅ›li wagi rosnÄ… do ogromnych wartoÅ›ci â†’ problem z treningiem.

**Tryby wyÅ›wietlania** (Settings â†’ Histograms â†’ Mode):
- **Offset** (domyÅ›lny): kolejne epoki narysowane jedna za drugÄ… w 3D â€” widaÄ‡ "ewolucjÄ™" rozkÅ‚adu w czasie.
- **Overlay**: wszystkie epoki naÅ‚oÅ¼one na siebie na jednym wykresie 2D.

#### 3. DISTRIBUTIONS â€” alternatywny widok histogramÃ³w

Ta sama informacja co HISTOGRAMS, ale w postaci **wstÄ™g percentylowych** (jak "wÄ…sy" na wykresie). Pokazuje medianÄ™, 1-szy i 3-ci kwartyl, min i max wartoÅ›ci w czasie. Åatwiejsze do odczytu gdy interesuje CiÄ™ ogÃ³lny trend, a nie dokÅ‚adny ksztaÅ‚t rozkÅ‚adu.

#### 4. TIME SERIES â€” dane czasowe

Alternatywny widok danych skalarnych z bardziej zaawansowanymi opcjami filtrowania i porÃ³wnywania.

### Interpretacja naszego treningu (circle_in_square)

Na podstawie wynikÃ³w z TensorBoard (widocznych na screenie):

1. **Gradients/Layer_FC1_Weights**: RozkÅ‚ad gradientÃ³w pierwszej warstwy â€” widaÄ‡ Å¼e na poczÄ…tku (epoka ~100-400) gradienty sÄ… wiÄ™ksze i bardziej "rozstrzelone", z czasem siÄ™ zwÄ™Å¼ajÄ…. To normalne â€” na poczÄ…tku sieÄ‡ duÅ¼o koryguje, potem coraz mniej, bo jest coraz bliÅ¼ej optimum.

2. **Gradients/Layer_FC2_Weights**: Podobny wzorzec jak FC1 â€” gradienty malejÄ… w czasie. Zakres wartoÅ›ci jest inny (od -0.003 do 0.007) => kaÅ¼da warstwa uczy siÄ™ w swoim "tempie".

3. **Gradients/Layer_FC3_Weights**: Warstwa wyjÅ›ciowa â€” gradienty sÄ… dodatnie i malejÄ… (od ~0.015 do mniejszych wartoÅ›ci). To logiczne â€” warstwa wyjÅ›ciowa bezpoÅ›rednio odpowiada za predykcjÄ™, wiÄ™c na poczÄ…tku dostaje najsilniejszy sygnaÅ‚ "popraw siÄ™".

4. **Loss** (widoczny poniÅ¼ej na screenie): Powinien pokazywaÄ‡ gÅ‚adki spadek od ~0.17 do ~0.004 â€” potwierdzenie Å¼e sieÄ‡ uczyÅ‚a siÄ™ stabilnie i skutecznie.

### Porady praktyczne

- **PorÃ³wnywanie przebiegÃ³w**: Uruchom trening z rÃ³Å¼nymi parametrami â€” TensorBoard pokaÅ¼e WSZYSTKIE przebiegi z folderu `runs/` jednoczeÅ›nie. MoÅ¼esz filtrowaÄ‡ po nazwie (panel po lewej).
- **Usuwanie starych logÃ³w**: JeÅ›li chcesz zaczÄ…Ä‡ "czysto", usuÅ„ folder `runs/` przed treningiem.
- **Logi z wielu sieci**: W tym projekcie XOR, binary-classification i circle-in-square zapisujÄ… do osobnych podfolderÃ³w `runs/` â€” moÅ¼esz porÃ³wnywaÄ‡ je jednoczeÅ›nie.
- **OdÅ›wieÅ¼anie**: TensorBoard nie odÅ›wieÅ¼a siÄ™ automatycznie â€” kliknij ikonÄ™ odÅ›wieÅ¼ania (strzaÅ‚ka w kÃ³Å‚ku, prawy gÃ³rny rÃ³g) lub odÅ›wieÅ¼ stronÄ™ w przeglÄ…darce.
