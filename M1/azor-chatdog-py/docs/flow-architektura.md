# ğŸ¶ Azor ChatDog â€” PeÅ‚na dokumentacja przepÅ‚ywu i architektury

> Dokument opisuje szczegÃ³Å‚owy przepÅ‚yw sterowania przez wszystkie klasy i moduÅ‚y aplikacji **Azor ChatDog** â€” interaktywnego chatbota CLI z obsÅ‚ugÄ… wielu silnikÃ³w LLM. Celem jest dogÅ‚Ä™bne zrozumienie zarÃ³wno generycznego flow, jak i szczegÃ³Å‚Ã³w implementacyjnych kaÅ¼dego z obsÅ‚ugiwanych modeli.

---

## Spis treÅ›ci

1. [PrzeglÄ…d architektury â€” widok z lotu ptaka](#1-przeglÄ…d-architektury--widok-z-lotu-ptaka)
2. [Punkt wejÅ›cia â€” `run.py` â†’ `chat.py`](#2-punkt-wejÅ›cia--runpy--chatpy)
3. [Warstwa CLI â€” `cli/`](#3-warstwa-cli--cli)
4. [ModuÅ‚ asystenta â€” `assistant/`](#4-moduÅ‚-asystenta--assistant)
5. [ZarzÄ…dzanie sesjami â€” `session/`](#5-zarzÄ…dzanie-sesjami--session)
6. [Warstwa klientÃ³w LLM â€” `llm/` (generyczny flow)](#6-warstwa-klientÃ³w-llm--llm-generyczny-flow)
7. [SzczegÃ³Å‚y implementacji: Gemini](#7-szczegÃ³Å‚y-implementacji-gemini)
8. [SzczegÃ³Å‚y implementacji: LLaMA (llama.cpp)](#8-szczegÃ³Å‚y-implementacji-llama-llamacpp)
9. [SzczegÃ³Å‚y implementacji: Ollama](#9-szczegÃ³Å‚y-implementacji-ollama)
10. [SzczegÃ³Å‚y implementacji: Anthropic (Claude)](#10-szczegÃ³Å‚y-implementacji-anthropic-claude)
11. [Walidacja konfiguracji â€” Pydantic](#11-walidacja-konfiguracji--pydantic)
12. [Warstwa plikÃ³w â€” `files/`](#12-warstwa-plikÃ³w--files)
13. [System komend â€” `commands/` + `command_handler.py`](#13-system-komend--commands--command_handlerpy)
14. [PeÅ‚ny flow wysÅ‚ania wiadomoÅ›ci â€” krok po kroku](#14-peÅ‚ny-flow-wysÅ‚ania-wiadomoÅ›ci--krok-po-kroku)
15. [Uniwersalny format historii](#15-uniwersalny-format-historii)
16. [PorÃ³wnanie klientÃ³w LLM â€” tabela zbiorcza](#16-porÃ³wnanie-klientÃ³w-llm--tabela-zbiorcza)
17. [Diagram zaleÅ¼noÅ›ci miÄ™dzy moduÅ‚ami](#17-diagram-zaleÅ¼noÅ›ci-miÄ™dzy-moduÅ‚ami)

---

## 1. PrzeglÄ…d architektury â€” widok z lotu ptaka

Aplikacja Azor ChatDog jest konsolowym chatbotem napisanym w Pythonie, ktÃ³ry obsÅ‚uguje **cztery rÃ³Å¼ne silniki LLM**:

| Silnik | Klasa klienta | Typ poÅ‚Ä…czenia | SDK / Biblioteka |
|--------|---------------|----------------|-------------------|
| **Gemini** | `GeminiLLMClient` | API chmurowe (Google) | `google-genai` |
| **LLaMA (llama.cpp)** | `LlamaClient` | Model lokalny (plik .gguf) | `llama-cpp-python` |
| **Ollama** | `OllamaClient` | Serwer lokalny (HTTP) | `ollama` (SDK) |
| **Anthropic (Claude)** | `AnthropicClient` | API chmurowe (Anthropic) | `anthropic` |

### Schemat warstw

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   run.py                        â”‚  â† Punkt wejÅ›cia
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   chat.py                       â”‚  â† GÅ‚Ã³wna pÄ™tla
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  cli/          â”‚  command_handler.py            â”‚  â† Interfejs uÅ¼ytkownika
â”‚  â”œâ”€ args.py    â”‚  commands/                     â”‚     i obsÅ‚uga komend
â”‚  â”œâ”€ console.py â”‚  â”œâ”€ welcome.py                 â”‚
â”‚  â””â”€ prompt.py  â”‚  â”œâ”€ session_list.py            â”‚
â”‚                â”‚  â”œâ”€ session_display.py          â”‚
â”‚                â”‚  â”œâ”€ session_summary.py          â”‚
â”‚                â”‚  â”œâ”€ session_remove.py           â”‚
â”‚                â”‚  â””â”€ session_to_pdf.py           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 session/                         â”‚  â† ZarzÄ…dzanie sesjami
â”‚  â”œâ”€ __init__.py (singleton SessionManager)       â”‚
â”‚  â”œâ”€ session_manager.py                           â”‚
â”‚  â””â”€ chat_session.py                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 assistant/                       â”‚  â† Definicja asystenta
â”‚  â”œâ”€ assistent.py (klasa Assistant)               â”‚
â”‚  â””â”€ azor.py (fabryka create_azor_assistant)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 llm/                             â”‚  â† Klienci LLM
â”‚  â”œâ”€ gemini_client.py      + gemini_validation.py â”‚
â”‚  â”œâ”€ llama_client.py       + llama_validation.py  â”‚
â”‚  â”œâ”€ ollama_client.py      + ollama_validation.py â”‚
â”‚  â””â”€ anthropic_client.py   + anthropic_validation â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 files/                           â”‚  â† Persystencja
â”‚  â”œâ”€ config.py (Å›cieÅ¼ki katalogÃ³w)                â”‚
â”‚  â”œâ”€ session_files.py (zapis/odczyt sesji JSON)   â”‚
â”‚  â”œâ”€ wal.py (Write-Ahead Log)                     â”‚
â”‚  â””â”€ pdf/ (generowanie PDF)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ZaleÅ¼noÅ›ci zewnÄ™trzne (requirements.txt)

| Pakiet | Wersja | Cel |
|--------|--------|-----|
| `google-genai` | latest | SDK Google Gemini |
| `python-dotenv` | latest | Åadowanie zmiennych z .env |
| `llama-cpp-python` | latest | Lokalne modele LLaMA (GGUF) |
| `ollama` | latest | SDK do serwera Ollama |
| `anthropic` | latest | SDK Anthropic Claude |
| `prompt_toolkit` | latest | Zaawansowany prompt CLI |
| `colorama` | latest | Kolorowe wyjÅ›cie terminala |
| `pydantic` | >=2.0.0 | Walidacja konfiguracji |
| `fpdf2` | latest | Generowanie plikÃ³w PDF |
| `markdown` | latest | Konwersja Markdown â†’ HTML (do PDF) |

---

## 2. Punkt wejÅ›cia â€” `run.py` â†’ `chat.py`

### `run.py` (6 linii)

```python
import chat

if __name__ == "__main__":
    chat.init_chat()
    chat.main_loop()
```

To jest absolutnie minimalny punkt wejÅ›cia. CaÅ‚a logika delegowana jest do moduÅ‚u `chat.py`.

### `chat.py` â€” inicjalizacja (`init_chat`)

Funkcja `init_chat()` wykonuje sekwencjÄ™ startowÄ…:

```
init_chat()
  â”‚
  â”œâ”€ 1. print_welcome()              â†’ WyÅ›wietla ASCII art psa Azora ("Woof Woof!")
  â”‚
  â”œâ”€ 2. get_session_manager()        â†’ Tworzy/pobiera singleton SessionManager
  â”‚
  â”œâ”€ 3. cli.args.get_session_id_from_cli()
  â”‚     â†’ Parsuje argumenty CLI: --session-id=<ID>
  â”‚     â†’ Zwraca str | None
  â”‚
  â”œâ”€ 4. manager.initialize_from_cli(cli_session_id)
  â”‚     â”œâ”€ JeÅ›li podano --session-id:
  â”‚     â”‚   â”œâ”€ Tworzy Assistant (create_azor_assistant)
  â”‚     â”‚   â”œâ”€ ChatSession.load_from_file(assistant, session_id)
  â”‚     â”‚   â”‚   â”œâ”€ session_files.load_session_history(session_id)
  â”‚     â”‚   â”‚   â””â”€ Tworzy ChatSession z zaÅ‚adowanÄ… historiÄ…
  â”‚     â”‚   â”‚       â””â”€ _initialize_llm_session() â† TU POWSTAJE KLIENT LLM!
  â”‚     â”‚   â””â”€ WyÅ›wietla pomoc + podsumowanie historii (jeÅ›li niepusta)
  â”‚     â””â”€ JeÅ›li NIE podano:
  â”‚         â”œâ”€ Tworzy nowy Assistant
  â”‚         â”œâ”€ Tworzy nowy ChatSession (nowy UUID)
  â”‚         â”‚   â””â”€ _initialize_llm_session() â† TU POWSTAJE KLIENT LLM!
  â”‚         â””â”€ WyÅ›wietla pomoc
  â”‚
  â””â”€ 5. atexit.register(manager.cleanup_and_save)
        â†’ Rejestruje handler zapisu przy wyjÅ›ciu z programu
```

**Kluczowy moment**: Klient LLM jest tworzony wewnÄ…trz `ChatSession._initialize_llm_session()`. To jest jedyne miejsce w kodzie, gdzie nastÄ™puje wybÃ³r silnika na podstawie zmiennej Å›rodowiskowej `ENGINE`.

### `chat.py` â€” gÅ‚Ã³wna pÄ™tla (`main_loop`)

```
main_loop()
  â”‚
  â””â”€ while True:
      â”‚
      â”œâ”€ get_user_input()              â†’ Prompt z auto-uzupeÅ‚nianiem (prompt_toolkit)
      â”‚
      â”œâ”€ JeÅ›li input pusty â†’ continue
      â”‚
      â”œâ”€ JeÅ›li input zaczyna siÄ™ od '/':
      â”‚   â””â”€ command_handler.handle_command(user_input)
      â”‚       â””â”€ Zwraca True â†’ break (wyjÅ›cie z pÄ™tli)
      â”‚
      â””â”€ W przeciwnym razie (rozmowa z modelem):
          â”œâ”€ session = manager.get_current_session()
          â”œâ”€ response = session.send_message(user_input)
          â”‚   â”œâ”€ _llm_chat_session.send_message(text)
          â”‚   â”œâ”€ Synchronizacja historii
          â”‚   â”œâ”€ Zapis do WAL (Write-Ahead Log)
          â”‚   â””â”€ return response
          â”œâ”€ (total_tokens, remaining, max) = session.get_token_info()
          â”œâ”€ console.print_assistant(f"AZOR: {response.text}")
          â”œâ”€ console.print_info(f"Tokens: {total} (PozostaÅ‚o: {remaining} / {max})")
          â””â”€ session.save_to_file()
              â””â”€ JeÅ›li bÅ‚Ä…d â†’ console.print_error(...)
```

**ObsÅ‚uga wyjÄ…tkÃ³w w pÄ™tli:**

| WyjÄ…tek | Reakcja |
|---------|---------|
| `KeyboardInterrupt` (Ctrl+C) | Komunikat + `break` â†’ atexit wywoÅ‚a `cleanup_and_save()` |
| `EOFError` (Ctrl+D) | Komunikat + `break` |
| `Exception` (ogÃ³lny) | Komunikat + traceback + `break` |

NiezaleÅ¼nie od sposobu wyjÅ›cia z pÄ™tli, handler `atexit` (zarejestrowany w `init_chat()`) zapewnia prÃ³bÄ™ finalnego zapisu sesji.

---

## 3. Warstwa CLI â€” `cli/`

### `cli/args.py` â€” Parsowanie argumentÃ³w

UÅ¼ywa standardowego `argparse`. Definiuje jeden opcjonalny argument:

| Argument | Typ | DomyÅ›lnie | Opis |
|----------|-----|-----------|------|
| `--session-id` | `str` | `None` | ID sesji do wznowienia (np. `a1b2c3d4`) |

Opis programu (w `--help`): *"Interaktywny pies asystent! ğŸ¶"*

**PrzykÅ‚ad uÅ¼ycia:**
```bash
python run.py --session-id=a1b2c3d4
```

### `cli/console.py` â€” Kolorowe wyjÅ›cie terminala

Centralizuje wyÅ›wietlanie tekstu w terminalu z uÅ¼yciem biblioteki `colorama`:

| Funkcja | Kolor | UÅ¼ycie |
|---------|-------|--------|
| `print_error(msg)` | ğŸ”´ Czerwony (`Fore.RED`) | Komunikaty o bÅ‚Ä™dach |
| `print_assistant(msg)` | ğŸŸ¦ Cyan (`Fore.CYAN`) | Odpowiedzi asystenta |
| `print_user(msg)` | ğŸ”µ Niebieski (`Fore.BLUE`) | WiadomoÅ›ci uÅ¼ytkownika (w wyÅ›wietlanej historii) |
| `print_info(msg)` | âšª DomyÅ›lny (brak koloru) | Informacje systemowe |
| `print_help(msg)` | ğŸŸ¡ Å»Ã³Å‚ty (`Fore.YELLOW`) | Komunikaty pomocy i komendy |

**ZÅ‚oÅ¼one funkcje wyÅ›wietlania:**

- `display_help(session_id)` â€” wyÅ›wietla:
  - Aktualne ID sesji
  - ÅšcieÅ¼kÄ™ katalogu logÃ³w (`~/.azor`)
  - PeÅ‚nÄ… listÄ™ dostÄ™pnych komend slash z opisami

- `display_final_instructions(session_id)` â€” wyÅ›wietla po wyjÅ›ciu:
  - InstrukcjÄ™ kontynuacji sesji:
  ```
  python run.py --session-id=<ID>
  ```
  - SformatowanÄ… jasnym biaÅ‚ym boldem (`Fore.WHITE + Style.BRIGHT`)

Inicjalizacja: `init(autoreset=True)` â€” automatyczny reset stylu po kaÅ¼dym princie.

### `cli/prompt.py` â€” Zaawansowany prompt wejÅ›ciowy

Wykorzystuje bibliotekÄ™ `prompt_toolkit` do zaawansowanego interfejsu wejÅ›ciowego. To najbardziej rozbudowany moduÅ‚ CLI:

**1. Kolorowanie skÅ‚adni â€” klasa `SlashCommandLexer(Lexer)`:**

Metoda `lex_document()` zwraca funkcjÄ™ `get_line_tokens(lineno)`, ktÃ³ra:
- Sprawdza czy linia zaczyna siÄ™ od jednej z komend: `/exit`, `/quit`, `/switch`, `/help`, `/session`
- JeÅ›li tak â†’ koloruje komendÄ™ na `#ff0066 bold` (rÃ³Å¼owo-czerwony, klasa `slash-command`)
- Dla `/session` â€” dodatkowo rozpoznaje podkomendy (`list`, `display`, `pop`, `clear`, `new`, `remove`) i koloruje je na `#00ff00 bold` (zielony, klasa `subcommand`)
- Reszta tekstu â†’ `#aaaaaa` (szary, klasa `normal-text`)

**2. Auto-uzupeÅ‚nianie â€” `NestedCompleter`:**

```python
_commands_completer = NestedCompleter({
    '/exit': None,
    '/quit': None,
    '/help': None,
    '/switch': None,
    '/session': WordCompleter(['list', 'display', 'pop', 'clear', 'new', 'remove'])
})
```

DziÄ™ki `NestedCompleter`, po wpisaniu `/session ` i Tab, uÅ¼ytkownik zobaczy podkomendy. Dla pozostaÅ‚ych komend brak podsugestii.

**3. Inteligentne zachowanie klawisza Enter â€” `KeyBindings`:**

```python
@kb.add('enter', filter=completion_is_selected)
def _(event):
    event.app.current_buffer.complete_state = None
```

Logika:
- Gdy dropdown auto-uzupeÅ‚niania jest otwarty i pozycja zaznaczona â†’ Enter **akceptuje sugestiÄ™** (zamyka menu, NIE wysyÅ‚a)
- Gdy dropdown jest zamkniÄ™ty â†’ domyÅ›lne zachowanie Entera â†’ **wysÅ‚anie promptu**

**Funkcja `get_user_input(prompt_text="TY: ")`** â€” Å‚Ä…czy wszystko razem:

```python
return prompt(
    prompt_text,                          # "TY: "
    completer=_commands_completer,        # auto-uzupeÅ‚nianie
    lexer=SlashCommandLexer(),            # kolorowanie skÅ‚adni
    style=_prompt_style,                  # definicje kolorÃ³w
    complete_while_typing=True,           # sugestie podczas pisania
    key_bindings=_key_bindings            # niestandardowe Enter
).strip()
```

---

## 4. ModuÅ‚ asystenta â€” `assistant/`

### `assistant/assistent.py` â€” Klasa `Assistant`

Prosta klasa enkapsulujÄ…ca konfiguracjÄ™ asystenta â€” jego toÅ¼samoÅ›Ä‡ i zachowanie:

```python
class Assistant:
    _system_prompt: str   # Prompt systemowy definiujÄ…cy zachowanie i osobowoÅ›Ä‡
    _name: str            # Nazwa wyÅ›wietlana w czacie (np. "AZOR")
```

**WÅ‚aÅ›ciwoÅ›ci (property):**
- `system_prompt` â†’ zwraca `_system_prompt` (read-only)
- `name` â†’ zwraca `_name` (read-only)

**Kluczowa cecha**: Klasa `Assistant` jest **caÅ‚kowicie niezaleÅ¼na od modelu LLM**. Definiuje jedynie *toÅ¼samoÅ›Ä‡* i *zachowanie* asystenta, nie implementacjÄ™ technicznÄ…. Ten sam asystent moÅ¼e byÄ‡ uÅ¼yty z dowolnym z czterech silnikÃ³w. To jest separacja **"co asystent robi"** od **"jak technicznie to robi"**.

### `assistant/azor.py` â€” Fabryka `create_azor_assistant()`

Tworzy konkretnÄ… instancjÄ™ asystenta "Azor":

| Parametr | WartoÅ›Ä‡ |
|----------|---------|
| `name` | `"AZOR"` |
| `system_prompt` | *"JesteÅ› pomocnym asystentem, Nazywasz siÄ™ Azor i jesteÅ› psem o wielkich moÅ¼liwoÅ›ciach. JesteÅ› najlepszym przyjacielem Reksia, ale chÄ™tnie nawiÄ…zujesz kontakt z ludÅºmi. Twoim zadaniem jest pomaganie uÅ¼ytkownikowi w rozwiÄ…zywaniu problemÃ³w, odpowiadanie na pytania i dostarczanie informacji w sposÃ³b uprzejmy i zrozumiaÅ‚y."* |

Fabryka `create_azor_assistant()` jest wywoÅ‚ywana w `SessionManager` za kaÅ¼dym razem, gdy tworzona jest nowa sesja lub Å‚adowana istniejÄ…ca. Wzorzec fabryki pozwala Å‚atwo dodaÄ‡ w przyszÅ‚oÅ›ci nowych asystentÃ³w z innymi promptami systemowymi (np. inny pies, inna osobowoÅ›Ä‡).

---

## 5. ZarzÄ…dzanie sesjami â€” `session/`

### `session/__init__.py` â€” Singleton `SessionManager`

ModuÅ‚ implementuje wzorzec **Singleton** (na poziomie moduÅ‚u) dla `SessionManager`:

```python
_session_manager: SessionManager | None = None

def get_session_manager() -> SessionManager:
    global _session_manager
    if _session_manager is None:
        _session_manager = SessionManager()
    return _session_manager
```

To gwarantuje, Å¼e w caÅ‚ej aplikacji istnieje **dokÅ‚adnie jedna instancja** managera sesji. KaÅ¼dy moduÅ‚ (chat.py, command_handler.py, komendy) wywoÅ‚uje `get_session_manager()` i dostaje tÄ™ samÄ… instancjÄ™.

**Eksportowane symbole:** `ChatSession`, `SessionManager`, `get_session_manager`.

### `session/session_manager.py` â€” Klasa `SessionManager`

Orkiestruje cykl Å¼ycia sesji i zarzÄ…dza aktywnÄ… sesjÄ…. To warstwa wyÅ¼szego poziomu nad `ChatSession` â€” odpowiada za przeÅ‚Ä…czanie miÄ™dzy sesjami, zapis przy przeÅ‚Ä…czaniu, cleanup przy wyjÅ›ciu, etc.

**Stan wewnÄ™trzny:**
```python
_current_session: ChatSession | None = None
```

#### Metoda `initialize_from_cli(cli_session_id)`:

```
initialize_from_cli(cli_session_id)
â”‚
â”œâ”€ cli_session_id != None (podano --session-id):
â”‚   â”œâ”€ assistant = create_azor_assistant()
â”‚   â”œâ”€ session, error = ChatSession.load_from_file(assistant, session_id)
â”‚   â”‚
â”‚   â”œâ”€ JeÅ›li error:
â”‚   â”‚   â”œâ”€ console.print_error(error)
â”‚   â”‚   â”œâ”€ Fallback: session = ChatSession(assistant=assistant)  â† nowa sesja
â”‚   â”‚   â””â”€ "RozpoczÄ™to nowÄ… sesjÄ™ z ID: ..."
â”‚   â”‚
â”‚   â”œâ”€ self._current_session = session
â”‚   â”œâ”€ display_help(session.session_id)
â”‚   â”œâ”€ JeÅ›li sesja ma historiÄ™:
â”‚   â”‚   â””â”€ display_history_summary(history, assistant_name)
â”‚   â””â”€ return session
â”‚
â””â”€ cli_session_id == None (nowa sesja):
    â”œâ”€ print("Rozpoczynanie nowej sesji.")
    â”œâ”€ assistant = create_azor_assistant()
    â”œâ”€ session = ChatSession(assistant=assistant)
    â”‚   â””â”€ __init__ â†’ _initialize_llm_session()  â† TU POWSTAJE KLIENT LLM
    â”œâ”€ self._current_session = session
    â”œâ”€ display_help(session.session_id)
    â””â”€ return session
```

#### Metoda `create_new_session(save_current=True)`:

```
create_new_session(save_current=True)
â”‚
â”œâ”€ JeÅ›li save_current i jest bieÅ¼Ä…ca sesja:
â”‚   â”œâ”€ save_attempted = True
â”‚   â”œâ”€ previous_session_id = current.session_id
â”‚   â””â”€ success, error = current.save_to_file()
â”‚       â””â”€ save_error = error jeÅ›li nie powiodÅ‚o siÄ™
â”‚
â”œâ”€ assistant = create_azor_assistant()
â”œâ”€ new_session = ChatSession(assistant=assistant)  â† nowy UUID, nowy klient LLM
â”œâ”€ self._current_session = new_session
â”‚
â””â”€ return (new_session, save_attempted, previous_session_id, save_error)
```

#### Metoda `switch_to_session(session_id)`:

```
switch_to_session(session_id)
â”‚
â”œâ”€ JeÅ›li jest bieÅ¼Ä…ca sesja:
â”‚   â”œâ”€ save_attempted = True
â”‚   â”œâ”€ previous_session_id = current.session_id
â”‚   â””â”€ current.save_to_file()  â† zapis bieÅ¼Ä…cej przed przeÅ‚Ä…czeniem
â”‚
â”œâ”€ assistant = create_azor_assistant()
â”œâ”€ new_session, error = ChatSession.load_from_file(assistant, session_id)
â”‚
â”œâ”€ JeÅ›li error:
â”‚   â””â”€ return (None, save_attempted, previous_id, False, error, False)
â”‚      â† _current_session NIE zmieniona! UÅ¼ytkownik zostaje w bieÅ¼Ä…cej sesji.
â”‚
â””â”€ JeÅ›li success:
    â”œâ”€ self._current_session = new_session
    â”œâ”€ has_history = not new_session.is_empty()
    â””â”€ return (new_session, save_attempted, previous_id, True, None, has_history)
```

#### Metoda `remove_current_session_and_create_new()`:

```
remove_current_session_and_create_new()
â”‚
â”œâ”€ Brak aktywnej sesji â†’ raise RuntimeError
â”‚
â”œâ”€ removed_session_id = current.session_id
â”œâ”€ session_files.remove_session_file(removed_session_id)
â”‚
â”œâ”€ ZAWSZE tworzy nowÄ… sesjÄ™ (nawet jeÅ›li usuwanie siÄ™ nie powiodÅ‚o):
â”‚   â”œâ”€ assistant = create_azor_assistant()
â”‚   â”œâ”€ new_session = ChatSession(assistant=assistant)
â”‚   â””â”€ self._current_session = new_session
â”‚
â””â”€ return (new_session, removed_session_id, remove_success, remove_error)
```

#### Metoda `cleanup_and_save()`:

```
cleanup_and_save()  â† WywoÅ‚ywane przez atexit
â”‚
â”œâ”€ Brak aktywnej sesji â†’ return
â”‚
â”œâ”€ session.is_empty() (< 2 wpisy):
â”‚   â””â”€ "Sesja jest pusta/niekompletna. PominiÄ™to finalny zapis."
â”‚
â””â”€ session NIE jest pusta:
    â”œâ”€ "Finalny zapis historii sesji: {session_id}"
    â”œâ”€ session.save_to_file()
    â””â”€ display_final_instructions(session_id)
```

### `session/chat_session.py` â€” Klasa `ChatSession`

**NajwaÅ¼niejsza klasa w aplikacji.** Enkapsuluje wszystko co dotyczy pojedynczej sesji czatu.

#### Stan wewnÄ™trzny:

```python
assistant: Assistant                                # Konfiguracja asystenta
session_id: str                                     # UUID sesji
_history: List[Any]                                 # Historia rozmowy (format uniwersalny)
_llm_client: Union[GeminiLLMClient, LlamaClient,
                   OllamaClient, AnthropicClient,
                   None] = None                     # Klient LLM (lazy init, singleton)
_llm_chat_session: Any | None = None                # Wrapper sesji czatu
_max_context_tokens: int = 32768                    # StaÅ‚y limit kontekstu
```

#### Mapowanie silnikÃ³w (`ENGINE_MAPPING`):

```python
ENGINE_MAPPING = {
    'LLAMA_CPP':  LlamaClient,
    'GEMINI':     GeminiLLMClient,
    'OLLAMA':     OllamaClient,
    'ANTHROPIC':  AnthropicClient,
}
```

#### Metoda `_initialize_llm_session()` â€” serce wyboru silnika:

```
_initialize_llm_session()
â”‚
â”œâ”€ 1. engine = os.getenv('ENGINE', 'GEMINI').upper()
â”‚
â”œâ”€ 2. Walidacja: engine in ENGINE_MAPPING?
â”‚     Nie â†’ raise ValueError("ENGINE musi byÄ‡: LLAMA_CPP, GEMINI, OLLAMA, ANTHROPIC")
â”‚
â”œâ”€ 3. JeÅ›li _llm_client == None (PIERWSZA inicjalizacja):
â”‚     â”œâ”€ SelectedClientClass = ENGINE_MAPPING[engine]
â”‚     â”œâ”€ console: "ğŸ¤– Przygotowywanie klienta..."
â”‚     â”œâ”€ self._llm_client = SelectedClientClass.from_environment()
â”‚     â””â”€ console: "âœ… Klient gotowy do uÅ¼ycia (...)"
â”‚
â””â”€ 4. self._llm_chat_session = self._llm_client.create_chat_session(
          system_instruction=self.assistant.system_prompt,
          history=self._history,
          thinking_budget=0
      )
```

**WaÅ¼ny niuans**: `_llm_client` tworzony **raz** (lazy init). Przy `clear_history()` i `pop_last_exchange()` tworzony jest tylko nowy `_llm_chat_session`.

#### Metoda `send_message(text)`:

```
send_message(text)
â”‚
â”œâ”€ Walidacja: _llm_chat_session?
â”œâ”€ response = _llm_chat_session.send_message(text)
â”œâ”€ self._history = _llm_chat_session.get_history()
â”œâ”€ total_tokens = self.count_tokens()
â”œâ”€ append_to_wal(session_id, text, response.text, total_tokens, model_name)
â”‚     â†’ WAL failure jest ignorowane (nie blokuje flow)
â””â”€ return response
```

#### Metoda `save_to_file()`:

```
save_to_file()
â”‚
â”œâ”€ Synchronizacja: _history = _llm_chat_session.get_history()
â””â”€ session_files.save_session_history(session_id, _history, system_prompt, model_name)
```

#### Metoda `load_from_file()` (classmethod):

```
ChatSession.load_from_file(assistant, session_id)
â”‚
â”œâ”€ history, error = session_files.load_session_history(session_id)
â”œâ”€ error â†’ return (None, error)
â””â”€ cls(assistant, session_id, history) â†’ _initialize_llm_session() z historiÄ…
```

#### PozostaÅ‚e metody:

| Metoda | Opis |
|--------|------|
| `get_history()` | Synchronizuje z `_llm_chat_session` i zwraca `_history` |
| `clear_history()` | `_history = []` â†’ `_initialize_llm_session()` â†’ `save_to_file()` |
| `pop_last_exchange()` | Sprawdza `len >= 2`, ucina `[-2:]`, reinicjalizuje, zapisuje |
| `count_tokens()` | Deleguje do `_llm_client.count_history_tokens(_history)` |
| `is_empty()` | `len(_history) < 2` |
| `get_remaining_tokens()` | `32768 - count_tokens()` |
| `get_token_info()` | `(total, remaining, max=32768)` |
| `assistant_name` (property) | `self.assistant.name` â†’ "AZOR" |

---

## 6. Warstwa klientÃ³w LLM â€” `llm/` (generyczny flow)

### WspÃ³lny kontrakt (implicit interface â€” duck typing)

Wszystkie cztery klasy klientÃ³w LLM implementujÄ… **identyczny zestaw metod**. Nie ma formalnej klasy bazowej (ABC) â€” kontrakt jest wymuszony przez **duck typing** w `ChatSession`:

```python
# Pseudo-interfejs â€” NIE istnieje w kodzie, ale jest niejawnie wymuszony
class LLMClient:
    @staticmethod
    def preparing_for_use_message() -> str: ...

    @classmethod
    def from_environment(cls) -> 'Self': ...

    def create_chat_session(self,
                            system_instruction: str,
                            history: Optional[List[Dict]] = None,
                            thinking_budget: int = 0) -> ChatSessionWrapper: ...

    def count_history_tokens(self, history: List[Dict]) -> int: ...

    def get_model_name(self) -> str: ...

    def is_available(self) -> bool: ...

    def ready_for_use_message(self) -> str: ...

    @property
    def client(self): ...  # Backwards compatibility
```

### WspÃ³lny kontrakt sesji czatu

```python
class ChatSessionWrapper:  # Pseudo-interfejs
    def send_message(self, text: str) -> Response: ...
    def get_history(self) -> List[Dict]: ...
```

### WspÃ³lny kontrakt odpowiedzi

KaÅ¼dy silnik ma swojÄ… klasÄ™ odpowiedzi, ale wszystkie majÄ… atrybut `.text: str`:
- **Gemini**: natywny obiekt Response z Google GenAI (ma `.text` natywnie)
- **LLaMA**: `LlamaResponse(text)` â€” prosta klasa z atrybutem `text`
- **Ollama**: `OllamaResponse(text)` â€” prosta klasa z atrybutem `text`
- **Anthropic**: `AnthropicResponse(text, input_tokens, output_tokens)` â€” rozszerzona o info o tokenach

### Generyczny flow tworzenia klienta (`from_environment()`)

```
SelectedClientClass.from_environment()
â”‚
â”œâ”€ load_dotenv()                         â†’ Åadowanie zmiennych z pliku .env
â”‚
â”œâ”€ Tworzenie Pydantic config:            â†’ Walidacja parametrÃ³w
â”‚   XxxConfig(
â”‚       model_name=os.getenv(..., default),
â”‚       api_key=os.getenv(...),
â”‚       ...dodatkowe parametry...
â”‚   )
â”‚   â”œâ”€ Walidacja typÃ³w (Pydantic BaseModel)
â”‚   â”œâ”€ Walidacja reguÅ‚ biznesowych (@validator)
â”‚   â””â”€ raise ValueError jeÅ›li niepoprawne
â”‚
â””â”€ cls(model_name=..., api_key=..., ...)
    â””â”€ __init__():
        â”œâ”€ Walidacja parametrÃ³w wejÅ›ciowych
        â”œâ”€ Zapisanie konfiguracji jako atrybuty instancji
        â””â”€ _initialize_client() / _initialize_model()
            â†’ SDK client / zaÅ‚adowany model
```

### Generyczny flow wysyÅ‚ania wiadomoÅ›ci

```
chat_session_wrapper.send_message(text)
â”‚
â”œâ”€ Dodanie user message do _history:
â”‚   {"role": "user", "parts": [{"text": text}]}
â”‚
â”œâ”€ Przygotowanie danych do wywoÅ‚ania:
â”‚   â”œâ”€ Gemini:    forward do natywnej sesji (auto-zarzÄ…dzanie historiÄ…)
â”‚   â”œâ”€ LLaMA:     _build_prompt_from_history() â†’ jeden string
â”‚   â”œâ”€ Ollama:    _build_messages_from_history() â†’ lista messages
â”‚   â””â”€ Anthropic: _build_messages_from_history() â†’ lista messages
â”‚
â”œâ”€ WywoÅ‚anie API / modelu
â”‚   â”œâ”€ success â†’ ekstrakcja tekstu
â”‚   â””â”€ error â†’ "Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d..."
â”‚
â”œâ”€ Dodanie model message do _history:
â”‚   {"role": "model", "parts": [{"text": response_text}]}
â”‚
â””â”€ return Response(text=response_text)
```

---

## 7. SzczegÃ³Å‚y implementacji: Gemini

### Klasy

| Klasa | Plik | Rola |
|-------|------|------|
| `GeminiLLMClient` | `gemini_client.py` | Klient API Google Gemini |
| `GeminiChatSessionWrapper` | `gemini_client.py` | Wrapper sesji â€” konwersja format historii |
| `GeminiConfig` | `gemini_validation.py` | Walidacja konfiguracji (Pydantic) |

### Konfiguracja

| Env Var | DomyÅ›lna wartoÅ›Ä‡ | Opis |
|---------|-----------------|------|
| `ENGINE` | `GEMINI` | Musi byÄ‡ `GEMINI` |
| `MODEL_NAME` | `gemini-2.5-flash` | Nazwa modelu |
| `GEMINI_API_KEY` | (wymagany) | Klucz API Google |

### Inicjalizacja klienta

```
GeminiLLMClient.from_environment()
â”‚
â”œâ”€ load_dotenv()
â”œâ”€ GeminiConfig(model_name=..., gemini_api_key=...)
â”‚   â””â”€ Walidacja: API key nie pusty (min_length=1 + @validator)
â””â”€ GeminiLLMClient(model_name, api_key)
    â””â”€ __init__():
        â”œâ”€ if not api_key â†’ raise ValueError
        â””â”€ _initialize_client() â†’ genai.Client()
            â”œâ”€ success â†’ return
            â””â”€ exception â†’ sys.exit(1)
```

**Uwaga**: `genai.Client()` bez argumentÃ³w â€” SDK szuka klucza w `GOOGLE_API_KEY` env var.

### Tworzenie sesji czatu

```
create_chat_session(system_instruction, history, thinking_budget=0)
â”‚
â”œâ”€ Konwersja historii: Dict â†’ types.Content
â”‚   entry â†’ types.Content(role=role, parts=[types.Part.from_text(text)])
â”‚
â”œâ”€ gemini_session = self._client.chats.create(
â”‚     model=self.model_name,
â”‚     history=gemini_history,
â”‚     config=types.GenerateContentConfig(
â”‚         system_instruction=system_instruction,
â”‚         thinking_config=types.ThinkingConfig(thinking_budget=0)
â”‚     )
â”‚   )
â”‚
â””â”€ return GeminiChatSessionWrapper(gemini_session)
```

**Cechy specyficzne Gemini:**
- System prompt jest **parametrem konfiguracyjnym** sesji, NIE wiadomoÅ›ciÄ… w historii
- ObsÅ‚uguje `thinking_budget` (extended reasoning) â€” tutaj wyÅ‚Ä…czony (0)
- Natywne **stateful sessions** â€” Gemini SDK zarzÄ…dza historiÄ… wewnÄ™trznie
- Wymaga **konwersji formatu** w obie strony (Dict â†” Content objects)

### `GeminiChatSessionWrapper`

**`send_message(text)`** â€” prosty forward:
```python
return self.gemini_session.send_message(text)
```
Gemini SDK sam dodaje user/model messages do wewnÄ™trznej historii. Response ma natywny `.text`.

**`get_history()`** â€” konwersja z powrotem:
```
Dla kaÅ¼dego Content w gemini_session.get_history():
  â”œâ”€ Szuka pierwszego part z text (nie pusty)
  â””â”€ â†’ {"role": content.role, "parts": [{"text": text_part}]}
```

### Liczenie tokenÃ³w

```python
response = self._client.models.count_tokens(model=self.model_name, contents=gemini_history)
return response.total_tokens
```

Gemini ma **natywne API do liczenia tokenÃ³w** (`models.count_tokens()`), ale z fallbackiem na heurystykÄ™ w razie bÅ‚Ä™dÃ³w.

### Emoji i komunikaty

- Preparing: `"ğŸ¤– Przygotowywanie klienta Gemini..."`
- Ready: `"âœ… Klient Gemini gotowy do uÅ¼ycia (Model: gemini-2.5-flash, Key: AIza...xY9z)"`

---

## 8. SzczegÃ³Å‚y implementacji: LLaMA (llama.cpp)

### Klasy

| Klasa | Plik | Rola |
|-------|------|------|
| `LlamaClient` | `llama_client.py` | Klient modelu lokalnego |
| `LlamaChatSession` | `llama_client.py` | Wrapper sesji â€” budowanie promptu tekstowego |
| `LlamaResponse` | `llama_client.py` | Prosta klasa response (`.text`) |
| `LlamaConfig` | `llama_validation.py` | Walidacja konfiguracji (Pydantic) |

### Konfiguracja

| Env Var | DomyÅ›lna wartoÅ›Ä‡ | Opis |
|---------|-----------------|------|
| `ENGINE` | - | Musi byÄ‡ `LLAMA_CPP` |
| `LLAMA_MODEL_NAME` | `llama-3.1-8b-instruct` | Nazwa wyÅ›wietlana |
| `LLAMA_MODEL_PATH` | (wymagany) | ÅšcieÅ¼ka do pliku `.gguf` |
| `LLAMA_GPU_LAYERS` | `1` | Ile warstw na GPU (offloading) |
| `LLAMA_CONTEXT_SIZE` | `2048` | Rozmiar okna kontekstu |

### Inicjalizacja klienta

```
LlamaClient.from_environment()
â”‚
â”œâ”€ load_dotenv()
â”œâ”€ LlamaConfig(model_name, llama_model_path, llama_gpu_layers, llama_context_size)
â”‚   â””â”€ Walidacja @validator('llama_model_path'):
â”‚       â”œâ”€ os.path.exists(v) â†’ ValueError jeÅ›li nie istnieje
â”‚       â””â”€ v.endswith('.gguf') â†’ ValueError jeÅ›li zÅ‚e rozszerzenie
â”‚
â”œâ”€ console: "Åadowanie modelu LLaMA z: {path}"
â”‚
â””â”€ LlamaClient(model_name, model_path, n_gpu_layers, n_ctx)
    â””â”€ __init__():
        â”œâ”€ if not model_path â†’ raise ValueError
        â”œâ”€ if not os.path.exists â†’ raise ValueError
        â””â”€ _initialize_model():
            â”œâ”€ console: "Inicjalizacja modelu LLaMA: ..."
            â””â”€ Llama(model_path, n_gpu_layers, n_ctx, verbose=False)
                â”œâ”€ success â†’ Llama instance
                â””â”€ exception â†’ raise RuntimeError
```

**Kluczowa rÃ³Å¼nica**: LLaMA Å‚aduje **caÅ‚y model do pamiÄ™ci RAM/VRAM**. To moÅ¼e trwaÄ‡ kilka-kilkanaÅ›cie sekund. `n_gpu_layers` kontroluje ile warstw offloadowaÄ‡ na GPU.

### Tworzenie sesji czatu

```
create_chat_session(system_instruction, history, thinking_budget=0)
â”‚
â”œâ”€ thinking_budget jest IGNOROWANY (parametr kompatybilnoÅ›ci)
â”‚
â””â”€ return LlamaChatSession(
      llama_model=self._llama_model,
      system_instruction=system_instruction,
      history=history or []
   )
```

**Brak konwersji historii** â€” LLaMA pracuje bezpoÅ›rednio na formacie uniwersalnym (Dict). Konwersja nastÄ™puje dopiero przy budowaniu promptu tekstowego.

### LlamaChatSession â€” szczegÃ³Å‚y

**`send_message(text)`:**

```
send_message(text)
â”‚
â”œâ”€ Dodanie do _history:
â”‚   {"role": "user", "parts": [{"text": text}]}
â”‚
â”œâ”€ prompt = _build_prompt_from_history()
â”‚
â”œâ”€ output = self.llama_model(
â”‚     prompt,
â”‚     max_tokens=512,                    â† staÅ‚e 512
â”‚     stop=["User:", "Assistant:",
â”‚           "\n\nUser:", "\n\nAssistant:"],
â”‚     echo=False
â”‚   )
â”‚
â”œâ”€ response_text = output["choices"][0]["text"].strip()
â”‚
â”œâ”€ Dodanie do _history:
â”‚   {"role": "model", "parts": [{"text": response_text}]}
â”‚
â””â”€ return LlamaResponse(response_text)
```

**`_build_prompt_from_history()` â€” kluczowa metoda:**

LLaMA nie ma natywnego API czatu â€” caÅ‚a rozmowa jest formatowana jako **jeden ciÄ…g tekstowy**:

```
System: {system_instruction}

User: {wiadomoÅ›Ä‡_1}
```

---

## 9. SzczegÃ³Å‚y implementacji: Ollama

### Klasy

| Klasa | Plik | Rola |
|-------|------|------|
| `OllamaClient` | `ollama_client.py` | Klient serwera Ollama |
| `OllamaChatSession` | `ollama_client.py` | Wrapper sesji â€” konwersja na format messages Ollama |
| `OllamaResponse` | `ollama_client.py` | Prosta klasa response (`.text`) |
| `OllamaConfig` | `ollama_validation.py` | Walidacja konfiguracji (Pydantic) |

### Architektura Ollama vs LLaMA

**Ollama â‰  LLaMA**. ChoÄ‡ Ollama moÅ¼e uruchamiaÄ‡ modele LLaMA, to:
- **LLaMA (llama.cpp)**: Å‚aduje model bezpoÅ›rednio do pamiÄ™ci procesu Python (in-process)
- **Ollama**: komunikuje siÄ™ z **osobnym serwerem** przez HTTP API (out-of-process)

Ollama wymaga uruchomionego serwera (`ollama serve`) w tle.

### Konfiguracja

| Env Var | DomyÅ›lna wartoÅ›Ä‡ | Opis |
|---------|-----------------|------|
| `ENGINE` | - | Musi byÄ‡ `OLLAMA` |
| `OLLAMA_MODEL_NAME` | `qwen2.5:7b-instruct` | Nazwa modelu w rejestrze Ollama |
| `OLLAMA_HOST` | `http://localhost:11434` | Adres serwera Ollama |

### Inicjalizacja klienta

```
OllamaClient.from_environment()
â”‚
â”œâ”€ load_dotenv()
â”œâ”€ OllamaConfig(model_name=..., ollama_host=...)
â”‚   â””â”€ Walidacja:
â”‚       â”œâ”€ model_name: nie pusty (@validator)
â”‚       â””â”€ ollama_host: nie pusty, http:// lub https://, strip + rstrip('/')
â”‚
â”œâ”€ console: "ÅÄ…czenie z serwerem Ollama: {host}"
â”‚
â””â”€ OllamaClient(model_name, host)
    â””â”€ __init__():
        â”œâ”€ if not model_name â†’ raise ValueError
        â””â”€ _initialize_client():
            â””â”€ OllamaSDKClient(host=self.host)
                â”œâ”€ success â†’ SDK client instance
                â””â”€ exception â†’ raise RuntimeError
```

### Tworzenie sesji czatu

```
create_chat_session(system_instruction, history, thinking_budget=0)
â”‚
â”œâ”€ thinking_budget IGNOROWANY (kompatybilnoÅ›Ä‡)
â”‚
â””â”€ return OllamaChatSession(
      ollama_client=self._client,
      model_name=self.model_name,
      system_instruction=system_instruction,
      history=history or []
   )
```

### `OllamaChatSession.send_message()` â€” API messages

```
send_message(text)
â”‚
â”œâ”€ Dodanie do _history:
â”‚   {"role": "user", "parts": [{"text": text}]}
â”‚
â”œâ”€ messages = _build_messages_from_history()
â”‚   â”‚
â”‚   â”‚  Konwersja na format Ollama:
â”‚   â”‚  [
â”‚   â”‚    {"role": "system",    "content": "{system_instruction}"},  â† pierwsza msg
â”‚   â”‚    {"role": "user",      "content": "{wiadomoÅ›Ä‡_1}"},
â”‚   â”‚    {"role": "assistant", "content": "{odpowiedÅº_1}"},
â”‚   â”‚    {"role": "user",      "content": "{wiadomoÅ›Ä‡_2}"},
â”‚   â”‚  ]
â”‚   â”‚
â”‚   â””â”€ Mapowanie rÃ³l: "user"â†’"user", "model"â†’"assistant"
â”‚      System prompt â†’ osobna wiadomoÅ›Ä‡ {"role": "system"} na poczÄ…tku
â”‚
â”œâ”€ response = self.ollama_client.chat(
â”‚     model=self.model_name,
â”‚     messages=messages,
â”‚     stream=False                       â† bez streamingu
â”‚   )
â”‚
â”œâ”€ response_text = response['message']['content'].strip()
â”‚
â”œâ”€ Dodanie do _history:
â”‚   {"role": "model", "parts": [{"text": response_text}]}
â”‚
â””â”€ return OllamaResponse(response_text)
```

**Kluczowe rÃ³Å¼nice vs LLaMA:**
1. **Prawdziwe API czatu** (chat API) â€” nie budowanie promptu tekstowego
2. **System prompt jako osobna wiadomoÅ›Ä‡** `{"role": "system", ...}` w tablicy messages
3. **Mapowanie rÃ³l**: `"model"` (wewnÄ™trzny) â†’ `"assistant"` (Ollama API)
4. **Bezstanowe** â€” peÅ‚na historia wysyÅ‚ana przy kaÅ¼dym requestcie
5. **`stream=False`** â€” odpowiedÅº w jednym kawaÅ‚ku (nie streaming)

### Liczenie tokenÃ³w â€” heurystyka

Ollama **nie ma natywnego API do zliczania tokenÃ³w**, wiÄ™c stosowana jest heurystyka:

```python
word_count = len(full_text.split())
return int(word_count * 0.75)   # ~0.75 tokena/sÅ‚owo (Å›rednia angielska)
```

Fallback: `total_chars // 4`.

### `is_available()` â€” sprawdzenie serwera

```python
def is_available(self):
    try:
        self._client.list()  # WywoÅ‚uje API listy modeli
        return True
    except Exception:
        return False
```

Ollama jest **jedynym klientem z aktywnym sprawdzaniem dostÄ™pnoÅ›ci serwera** (wysyÅ‚a faktyczny request HTTP).

### Emoji i komunikaty

- Preparing: `"ğŸ¦™ Przygotowywanie klienta Ollama..."`
- Ready: `"âœ… Klient Ollama gotowy do uÅ¼ycia (Model: qwen2.5:7b-instruct, Host: http://localhost:11434)"`

---

## 10. SzczegÃ³Å‚y implementacji: Anthropic (Claude)

### Klasy

| Klasa | Plik | Rola |
|-------|------|------|
| `AnthropicClient` | `anthropic_client.py` | Klient API Anthropic Claude |
| `AnthropicChatSession` | `anthropic_client.py` | Wrapper sesji â€” konwersja + tracking tokenÃ³w per request |
| `AnthropicResponse` | `anthropic_client.py` | Response z `.text` + `.input_tokens` + `.output_tokens` |
| `AnthropicConfig` | `anthropic_validation.py` | Walidacja konfiguracji (Pydantic) |

### Konfiguracja

| Env Var | DomyÅ›lna wartoÅ›Ä‡ | Opis |
|---------|-----------------|------|
| `ENGINE` | - | Musi byÄ‡ `ANTHROPIC` |
| `ANTHROPIC_MODEL_NAME` | `claude-haiku-4-5-20251001` | Nazwa modelu Claude |
| `ANTHROPIC_API_KEY` | (wymagany) | Klucz API Anthropic |
| `ANTHROPIC_MAX_TOKENS` | `4096` | Maks. tokenÃ³w w odpowiedzi |

### Inicjalizacja klienta

```
AnthropicClient.from_environment()
â”‚
â”œâ”€ load_dotenv()
â”œâ”€ AnthropicConfig(model_name=..., anthropic_api_key=..., max_tokens=...)
â”‚   â””â”€ Walidacja:
â”‚       â”œâ”€ API key: nie pusty (min_length=1 + @validator strip)
â”‚       â”œâ”€ model_name: nie pusty (@validator strip)
â”‚       â””â”€ max_tokens: >= 1
â”‚
â””â”€ AnthropicClient(model_name, api_key, max_tokens)
    â””â”€ __init__():
        â”œâ”€ if not api_key â†’ raise ValueError
        â””â”€ _initialize_client():
            â””â”€ Anthropic(api_key=self.api_key)  â† klucz jawnie!
                â”œâ”€ success â†’ Anthropic SDK client
                â””â”€ exception â†’ sys.exit(1)
```

**W przeciwieÅ„stwie do Gemini**, Anthropic client dostaje klucz API **jawnie** w konstruktorze `Anthropic(api_key=...)`.

### Tworzenie sesji czatu

```
create_chat_session(system_instruction, history, thinking_budget=0)
â”‚
â”œâ”€ thinking_budget IGNOROWANY (kompatybilnoÅ›Ä‡)
â”‚
â””â”€ return AnthropicChatSession(
      anthropic_client=self._client,
      model_name=self.model_name,
      system_instruction=system_instruction,
      max_tokens=self.max_tokens,          â† 4096 domyÅ›lnie
      history=history or []
   )
```

### `AnthropicChatSession` â€” dodatkowy stan

```python
_total_input_tokens: int = 0
_total_output_tokens: int = 0
```

Anthropic jest **jedynym silnikiem z kumulatywnym trackingiem tokenÃ³w per sesja**.

### `AnthropicChatSession.send_message()` â€” API Messages

```
send_message(text)
â”‚
â”œâ”€ Dodanie do _history:
â”‚   {"role": "user", "parts": [{"text": text}]}
â”‚
â”œâ”€ messages = _build_messages_from_history()
â”‚   â”‚
â”‚   â”‚  Konwersja na format Anthropic:
â”‚   â”‚  [
â”‚   â”‚    {"role": "user",      "content": "{wiadomoÅ›Ä‡_1}"},
â”‚   â”‚    {"role": "assistant", "content": "{odpowiedÅº_1}"},
â”‚   â”‚    {"role": "user",      "content": "{wiadomoÅ›Ä‡_2}"},
â”‚   â”‚  ]
â”‚   â”‚
â”‚   â”‚  UWAGA: System prompt NIE jest w messages[]!
â”‚   â”‚  Mapowanie rÃ³l: "user"â†’"user", "model"â†’"assistant"
â”‚
â”œâ”€ response = self.anthropic_client.messages.create(
â”‚     model=self.model_name,
â”‚     max_tokens=self.max_tokens,         â† konfigurowalny (4096)
â”‚     system=self.system_instruction,     â† OSOBNY parametr top-level!
â”‚     messages=cast(Iterable, messages)
â”‚   )
â”‚
â”œâ”€ Ekstrakcja tekstu z content blocks:
â”‚   for block in response.content:
â”‚     if block.type == "text":
â”‚       response_text += block.text
â”‚
â”œâ”€ Tracking tokenÃ³w (unikalne dla Anthropic):
â”‚   input_tokens = response.usage.input_tokens
â”‚   output_tokens = response.usage.output_tokens
â”‚   self._total_input_tokens += input_tokens
â”‚   self._total_output_tokens += output_tokens
â”‚
â”œâ”€ Dodanie do _history:
â”‚   {"role": "model", "parts": [{"text": response_text}]}
â”‚
â””â”€ return AnthropicResponse(response_text, input_tokens, output_tokens)
```

**Kluczowe cechy Anthropic:**

1. **System prompt jako parametr `system=`** â€” NIE jako wiadomoÅ›Ä‡ w `messages[]`. To waÅ¼na rÃ³Å¼nica vs Ollama, ktÃ³re wstawia system prompt jako pierwszÄ… wiadomoÅ›Ä‡ w tablicy.

2. **`max_tokens` jest wymagany** â€” Anthropic API wymaga jawnego podania limitu tokenÃ³w odpowiedzi (domyÅ›lnie 4096).

3. **Content blocks** â€” OdpowiedÅº Anthropic to lista blokÃ³w (`"text"`, `"tool_use"`, etc.). Iterujemy po blokach typu `"text"`.

4. **Tracking tokenÃ³w** â€” `response.usage.input_tokens` i `output_tokens` dajÄ… precyzyjne dane per request. Kumulowane w sesji.

5. **`cast(Iterable, messages)`** â€” type hint dla Pythona (API oczekuje `Iterable`).

### Liczenie tokenÃ³w â€” natywne API z fallbackiem

```
count_history_tokens(history)
â”‚
â”œâ”€ TRY: self._client.messages.count_tokens(
â”‚     model=self.model_name,
â”‚     messages=messages  â† skonwertowane na format Anthropic
â”‚   )
â”‚   â””â”€ return response.input_tokens
â”‚
â””â”€ EXCEPT (fallback heurystyka):
    word_count * 0.75
```

Anthropic ma **natywne API do liczenia tokenÃ³w** (`messages.count_tokens()`), z automatycznym fallbackiem na heurystykÄ™.

### `AnthropicResponse` â€” rozszerzony response

```python
class AnthropicResponse:
    text: str                 # Tekst odpowiedzi
    input_tokens: int = 0     # Tokeny wejÅ›ciowe tego requestu
    output_tokens: int = 0    # Tokeny wyjÅ›ciowe tego requestu
```

Jedyny response z per-request informacjami o zuÅ¼yciu tokenÃ³w.

### Emoji i komunikaty

- Preparing: `"ğŸ§  Przygotowywanie klienta Anthropic..."`
- Ready: `"âœ… Klient Anthropic gotowy do uÅ¼ycia (Model: claude-haiku-4-5-20251001, Key: sk-a...Yz9z)"`

---

## 11. Walidacja konfiguracji â€” Pydantic

KaÅ¼dy silnik ma dedykowany model Pydantic (`BaseModel`) do walidacji konfiguracji z zmiennych Å›rodowiskowych. Walidacja odbywa siÄ™ **przed** tworzeniem klienta â€” zasada fail-fast.

### `GeminiConfig`

```python
class GeminiConfig(BaseModel):
    engine: Literal["GEMINI"] = "GEMINI"
    model_name: str                              # np. "gemini-2.5-flash"
    gemini_api_key: str  # min_length=1

    @validator('gemini_api_key')
    def validate_api_key(cls, v):
        if not v or v.strip() == "":
            raise ValueError("GEMINI_API_KEY nie moÅ¼e byÄ‡ pusty")
        return v.strip()
```

### `LlamaConfig`

```python
class LlamaConfig(BaseModel):
    engine: Literal["LLAMA"] = "LLAMA"
    model_name: str
    llama_model_path: str                        # Å›cieÅ¼ka do .gguf
    llama_gpu_layers: int = 1     # ge=0
    llama_context_size: int = 2048  # ge=1

    @validator('llama_model_path')
    def validate_model_path(cls, v):
        if not os.path.exists(v):
            raise ValueError(f"Plik modelu nie istnieje: {v}")
        if not v.endswith('.gguf'):
            raise ValueError("Plik modelu musi mieÄ‡ rozszerzenie .gguf")
        return v
```

**Najsurowsza walidacja** â€” sprawdza istnienie pliku na dysku i rozszerzenie.

### `OllamaConfig`

```python
class OllamaConfig(BaseModel):
    engine: Literal["OLLAMA"] = "OLLAMA"
    model_name: str
    ollama_host: str = "http://localhost:11434"

    @validator('model_name')  # nie pusty, strip
    @validator('ollama_host')  # nie pusty, http(s)://, strip + rstrip('/')
```

### `AnthropicConfig`

```python
class AnthropicConfig(BaseModel):
    engine: Literal["ANTHROPIC"] = "ANTHROPIC"
    model_name: str
    anthropic_api_key: str  # min_length=1
    max_tokens: int = 4096  # ge=1

    @validator('anthropic_api_key')   # nie pusty, strip
    @validator('model_name')          # nie pusty, strip
```

### PorÃ³wnanie walidacji per silnik

| Aspekt | Gemini | LLaMA | Ollama | Anthropic |
|--------|--------|-------|--------|-----------|
| API Key | âœ… wymagany | âŒ brak | âŒ brak | âœ… wymagany |
| Model name | âœ… (z default) | âœ… (z default) | âœ… (z default) | âœ… (z default) |
| ÅšcieÅ¼ka pliku | âŒ | âœ… istnienie + .gguf | âŒ | âŒ |
| Host URL | âŒ | âŒ | âœ… http(s):// | âŒ |
| Max tokens | âŒ | âŒ | âŒ | âœ… ge=1 |
| GPU layers | âŒ | âœ… ge=0 | âŒ | âŒ |
| Context size | âŒ | âœ… ge=1 | âŒ | âŒ |

---

## 12. Warstwa plikÃ³w â€” `files/`

### `files/config.py` â€” Konfiguracja Å›cieÅ¼ek

```python
LOG_DIR    = ~/.azor/                    # Katalog sesji i WAL
OUTPUT_DIR = ~/.azor/output/             # Katalog wyjÅ›ciowy (PDF)
WAL_FILE   = ~/.azor/azor-wal.json       # Plik Write-Ahead Log
```

Katalogi tworzone automatycznie przy importcie (`os.makedirs(exist_ok=True)`). `load_dotenv()` wywoÅ‚ywane tutaj rÃ³wnieÅ¼.

### `files/session_files.py` â€” Operacje na plikach sesji

#### `load_session_history(session_id)` â†’ `(List[Dict], str | None)`

```
load_session_history(session_id)
â”‚
â”œâ”€ log_filename = ~/.azor/{session_id}-log.json
â”œâ”€ Nie istnieje â†’ ([], "Session log file does not exist...")
â”œâ”€ JSONDecodeError â†’ ([], "Cannot decode log file...")
â””â”€ Konwersja z JSON na format uniwersalny:
    {"role": ..., "timestamp": ..., "text": ...}
                        â†“
    {"role": role, "parts": [{"text": text}]}
```

#### `save_session_history(session_id, history, system_prompt, model_name)` â†’ `(bool, str | None)`

```
save_session_history(...)
â”‚
â”œâ”€ len(history) < 2 â†’ (True, None) â€” nie zapisuj pustych sesji
â”œâ”€ Konwersja: Dict â†’ JSON z timestamp
â””â”€ json.dump() â†’ ~/.azor/{session_id}-log.json
```

**Format pliku JSON na dysku:**
```json
{
    "session_id": "a1b2c3d4-...",
    "model": "gemini-2.5-flash",
    "system_role": "JesteÅ› pomocnym asystentem...",
    "history": [
        {"role": "user",  "timestamp": "2026-02-22T10:00:00", "text": "CzeÅ›Ä‡!"},
        {"role": "model", "timestamp": "2026-02-22T10:00:01", "text": "Hau hau!"}
    ]
}
```

`ensure_ascii=False` pozwala na zapis polskich znakÃ³w.

#### `list_sessions()` â†’ `List[Dict]`

Skanuje `~/.azor/` po plikach `*-log.json`. Zwraca metadane: ID, liczba wiadomoÅ›ci, data ostatniej aktywnoÅ›ci.

#### `remove_session_file(session_id)` â†’ `(bool, str | None)`

Proste `os.remove()` z obsÅ‚ugÄ… bÅ‚Ä™dÃ³w (plik nie istnieje, OSError).

### `files/wal.py` â€” Write-Ahead Log

WAL to dodatkowa warstwa bezpieczeÅ„stwa â€” zapisuje **kaÅ¼dÄ… interakcjÄ™ natychmiast**, niezaleÅ¼nie od gÅ‚Ã³wnego zapisu sesji.

```
append_to_wal(session_id, prompt, response_text, total_tokens, model_name)
â”‚
â”œâ”€ wal_entry = {
â”‚     "timestamp": now().isoformat(),
â”‚     "session_id": session_id,
â”‚     "model": model_name,
â”‚     "prompt": prompt,
â”‚     "response": response_text,
â”‚     "tokens_used": total_tokens
â”‚   }
â”‚
â”œâ”€ Odczyt istniejÄ…cego WAL â†’ JSONDecodeError â†’ reset
â”œâ”€ data.append(wal_entry)
â””â”€ json.dump(data) â†’ ~/.azor/azor-wal.json
```

**Cel WAL-a**: Nawet jeÅ›li gÅ‚Ã³wny plik sesji nie zostanie zapisany (crash, kill), WAL zawiera historiÄ™ wszystkich interakcji. Jest to **append-only log** w jednym pliku globalnym.

### `files/pdf/pdf.py` â€” Generowanie PDF

```
generate_pdf_from_markdown(markdown_content, output_filename)
â”‚
â”œâ”€ FPDF() â€” tworzenie dokumentu
â”œâ”€ Åadowanie czcionek Lato (Regular, Bold, Italic, BoldItalic)
â”œâ”€ Markdown â†’ HTML (biblioteka markdown)
â”œâ”€ OwiniÄ™cie HTML w <font face="Lato">...</font>
â”œâ”€ pdf.write_html(html_template)
â””â”€ pdf.output(~/.azor/output/{filename})
```

---

## 13. System komend â€” `commands/` + `command_handler.py`

### `command_handler.py` â€” Router komend

Centralne miejsce obsÅ‚ugi komend slash:

```python
VALID_SLASH_COMMANDS = ['/exit', '/quit', '/switch', '/help', '/session', '/pdf']
```

**`handle_command(user_input) â†’ bool`** (True = wyjÅ›cie z pÄ™tli):

| Komenda | DziaÅ‚anie |
|---------|-----------|
| `/help` | `display_help(session_id)` |
| `/exit`, `/quit` | Komunikat + `return True` |
| `/switch <ID>` | `manager.switch_to_session(ID)` |
| `/session <sub>` | Delegacja do `handle_session_subcommand()` |
| `/pdf` | `export_session_to_pdf()` |
| Inna | Komunikat o bÅ‚Ä™dzie + help |

**`handle_session_subcommand(subcommand, manager)`:**

| Podkomenda | DziaÅ‚anie |
|------------|-----------|
| `list` | `list_sessions_command()` â€” lista sesji z metadanymi |
| `display` | `display_full_session()` â€” numerowana peÅ‚na historia |
| `pop` | `current.pop_last_exchange()` â€” usuniÄ™cie ostatniej pary user+model |
| `clear` | `current.clear_history()` â€” wyczyszczenie historii |
| `new` | `manager.create_new_session()` â€” zapisanie bieÅ¼Ä…cej i nowa sesja |
| `remove` | `remove_session_command()` â€” usuniÄ™cie pliku sesji + nowa sesja |

### Pliki komend (`commands/`)

| Plik | Funkcja | Opis |
|------|---------|------|
| `welcome.py` | `print_welcome()` | ASCII art psa z dymkiem "Woof Woof!" |
| `session_list.py` | `list_sessions_command()` | Lista sesji z `session_files.list_sessions()` |
| `session_display.py` | `display_full_session()` | Numerowana historia: `[1] TY: ...`, `[2] AZOR: ...` |
| `session_summary.py` | `display_history_summary()` | Ostatnie 2 wiadomoÅ›ci + info o pominiÄ™tych |
| `session_remove.py` | `remove_session_command()` | UsuniÄ™cie pliku + nowa sesja |
| `session_to_pdf.py` | `export_session_to_pdf()` | Historia â†’ Markdown â†’ PDF |

---

## 14. PeÅ‚ny flow wysÅ‚ania wiadomoÅ›ci â€” krok po kroku

PoniÅ¼ej peÅ‚ny, liniowy flow od momentu wpisania tekstu do wyÅ›wietlenia odpowiedzi â€” z zaznaczeniem, gdzie kaÅ¼dy silnik siÄ™ rÃ³Å¼ni:

```
 1. UÅ¼ytkownik wpisuje tekst w prompt_toolkit
    prompt("TY: ", completer=..., lexer=...) â†’ "CzeÅ›Ä‡, Azor!"

 2. chat.main_loop() otrzymuje user_input = "CzeÅ›Ä‡, Azor!"

 3. Tekst NIE zaczyna siÄ™ od '/' â†’ gaÅ‚Ä…Åº rozmowy

 4. session = manager.get_current_session() â†’ aktywna ChatSession

 5. response = session.send_message("CzeÅ›Ä‡, Azor!")
    â”‚
    â”‚  5a. _llm_chat_session.send_message("CzeÅ›Ä‡, Azor!")
    â”‚      â”‚
    â”‚      â”‚  [GEMINI]:
    â”‚      â”‚  â””â”€ gemini_session.send_message(text) â†’ Google API call
    â”‚      â”‚     â†’ SDK automatycznie zarzÄ…dza historiÄ…
    â”‚      â”‚     â†’ return natywny Response
    â”‚      â”‚
    â”‚      â”‚  [LLAMA]:
    â”‚      â”‚  â”œâ”€ Dodaje user msg do _history
    â”‚      â”‚  â”œâ”€ _build_prompt_from_history()
    â”‚      â”‚  â”‚   â†’ "System: ...\n\nUser: CzeÅ›Ä‡, Azor!\n\nAssistant:"
    â”‚      â”‚  â”œâ”€ llama_model(prompt, max_tokens=512, stop=[...])
    â”‚      â”‚  â”œâ”€ Dodaje model msg do _history
    â”‚      â”‚  â””â”€ return LlamaResponse(text)
    â”‚      â”‚
    â”‚      â”‚  [OLLAMA]:
    â”‚      â”‚  â”œâ”€ Dodaje user msg do _history
    â”‚      â”‚  â”œâ”€ _build_messages_from_history()
    â”‚      â”‚  â”‚   â†’ [{"role":"system",...}, {"role":"user","content":"CzeÅ›Ä‡, Azor!"}]
    â”‚      â”‚  â”œâ”€ ollama_client.chat(model=..., messages=..., stream=False)
    â”‚      â”‚  â”œâ”€ Dodaje model msg do _history
    â”‚      â”‚  â””â”€ return OllamaResponse(text)
    â”‚      â”‚
    â”‚      â”‚  [ANTHROPIC]:
    â”‚      â”‚  â”œâ”€ Dodaje user msg do _history
    â”‚      â”‚  â”œâ”€ _build_messages_from_history()
    â”‚      â”‚  â”‚   â†’ [{"role":"user","content":"CzeÅ›Ä‡, Azor!"}]
    â”‚      â”‚  â”œâ”€ client.messages.create(model=..., max_tokens=4096,
    â”‚      â”‚  â”‚     system=system_instruction, messages=messages)
    â”‚      â”‚  â”œâ”€ Ekstrakcja z content blocks + tracking tokenÃ³w
    â”‚      â”‚  â”œâ”€ Dodaje model msg do _history
    â”‚      â”‚  â””â”€ return AnthropicResponse(text, in_tokens, out_tokens)
    â”‚
    â”‚  5b. self._history = _llm_chat_session.get_history()
    â”‚      [GEMINI]: Konwersja Content â†’ Dict
    â”‚      [Inne]:   Zwrot bezpoÅ›rednio _history (juÅ¼ Dict)
    â”‚
    â”‚  5c. total_tokens = count_tokens()
    â”‚      [GEMINI]:    client.models.count_tokens() â† precyzyjne API
    â”‚      [LLAMA]:     llama_model.tokenize() â† wbudowany tokenizer
    â”‚      [OLLAMA]:    word_count * 0.75 â† heurystyka
    â”‚      [ANTHROPIC]: TRY count_tokens() EXCEPT heurystyka
    â”‚
    â”‚  5d. append_to_wal(...) â†’ ~/.azor/azor-wal.json

 6. (total, remaining, max) = session.get_token_info()

 7. console.print_assistant("AZOR: Hau hau! Jestem Azor!")  â†’ CYAN

 8. console.print_info("Tokens: 150 (PozostaÅ‚o: 32618 / 32768)")

 9. session.save_to_file()
    â†’ session_files.save_session_history(...)
    â†’ ~/.azor/{session_id}-log.json

10. PowrÃ³t do pÄ™tli â†’ czekanie na kolejny input
```

---

## 15. Uniwersalny format historii

Jednym z kluczowych rozwiÄ…zaÅ„ architektonicznych jest **uniwersalny format historii** â€” jednolita struktura danych niezaleÅ¼na od silnika LLM.

### Format w pamiÄ™ci (Python Dict)

```python
{"role": "user" | "model", "parts": [{"text": "treÅ›Ä‡ wiadomoÅ›ci"}]}
```

### Konwersje per silnik

```
                    Uniwersalny format (Dict)
                    {"role": "user|model", "parts": [{"text": "..."}]}
                           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                â”‚                                â”‚
     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
     â”‚  Gemini   â”‚   â”‚   LLaMA    â”‚   â”‚   Ollama    â”‚  â”‚Anthropicâ”‚
     â”‚ Content   â”‚   â”‚  Prompt    â”‚   â”‚  Messages   â”‚  â”‚Messages â”‚
     â”‚ objects   â”‚   â”‚  string    â”‚   â”‚   list      â”‚  â”‚  list   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Gemini:    types.Content(role=..., parts=[types.Part.from_text(...)])
LLaMA:     "System: ...\n\nUser: ...\n\nAssistant: ...\n\nAssistant:"
Ollama:    [{"role": "system|user|assistant", "content": "..."}]
Anthropic: [{"role": "user|assistant", "content": "..."}] + system= osobno
```

### Format na dysku (JSON)

```json
{"role": "user", "timestamp": "2026-02-22T10:00:00", "text": "treÅ›Ä‡"}
```

Konwersja przy zapisie: `parts[0]["text"]` â†’ `text` + dodanie `timestamp`.
Konwersja przy odczycie: `text` â†’ `parts: [{"text": text}]`.

---

## 16. PorÃ³wnanie klientÃ³w LLM â€” tabela zbiorcza

| Cecha | Gemini | LLaMA (llama.cpp) | Ollama | Anthropic |
|-------|--------|-------------------|--------|-----------|
| **ENGINE env** | `GEMINI` | `LLAMA_CPP` | `OLLAMA` | `ANTHROPIC` |
| **Typ poÅ‚Ä…czenia** | API chmurowe | Model lokalny (in-process) | Serwer lokalny (HTTP) | API chmurowe |
| **SDK** | `google-genai` | `llama-cpp-python` | `ollama` | `anthropic` |
| **Wymaga klucza API** | âœ… `GEMINI_API_KEY` | âŒ | âŒ | âœ… `ANTHROPIC_API_KEY` |
| **Wymaga serwera** | âŒ (chmura) | âŒ (in-process) | âœ… `ollama serve` | âŒ (chmura) |
| **Wymaga pliku modelu** | âŒ | âœ… `.gguf` | âŒ | âŒ |
| **System prompt** | Parametr konfiguracji | CzÄ™Å›Ä‡ promptu tekstowego | WiadomoÅ›Ä‡ `system` | Osobny parametr `system=` |
| **ZarzÄ…dzanie historiÄ…** | Natywne (stateful SDK) | Manualne (wrapper) | Manualne (wrapper) | Manualne (wrapper) |
| **Format do API** | `Content` objects | Jeden string prompt | Lista messages | Lista messages |
| **max_tokens** | Dynamiczny | 512 (hardcoded) | DomyÅ›lny Ollama | Konfigurowalny (4096) |
| **Liczenie tokenÃ³w** | âœ… Natywne API | âœ… Tokenizer modelu | âŒ Heurystyka | âœ…/âŒ API + fallback |
| **Tracking per request** | âŒ | âŒ | âŒ | âœ… (input + output) |
| **thinking_budget** | âœ… ObsÅ‚ugiwany | âŒ Ignorowany | âŒ Ignorowany | âŒ Ignorowany |
| **Emoji** | ğŸ¤– | ğŸ¦™ | ğŸ¦™ | ğŸ§  |
| **DomyÅ›lny model** | `gemini-2.5-flash` | `llama-3.1-8b-instruct` | `qwen2.5:7b-instruct` | `claude-haiku-4-5-20251001` |
| **ObsÅ‚uga bÅ‚Ä™dÃ³w init** | `sys.exit(1)` | `raise RuntimeError` | `raise RuntimeError` | `sys.exit(1)` |

---

## 17. Diagram zaleÅ¼noÅ›ci miÄ™dzy moduÅ‚ami

```
run.py
  â””â”€ chat.py
       â”œâ”€ cli/args.py                    (parsowanie argumentÃ³w)
       â”œâ”€ cli/prompt.py                  (input uÅ¼ytkownika)
       â”œâ”€ cli/console.py                 (kolorowe output)
       â”œâ”€ commands/welcome.py            (ASCII art)
       â”œâ”€ command_handler.py
       â”‚    â”œâ”€ commands/session_list.py
       â”‚    â”‚    â””â”€ files/session_files.py
       â”‚    â”œâ”€ commands/session_display.py
       â”‚    â”œâ”€ commands/session_summary.py
       â”‚    â”œâ”€ commands/session_remove.py
       â”‚    â”‚    â””â”€ session/session_manager.py
       â”‚    â””â”€ commands/session_to_pdf.py
       â”‚         â””â”€ files/pdf/pdf.py
       â”‚              â””â”€ files/config.py
       â”‚
       â””â”€ session/ (singleton get_session_manager)
            â”œâ”€ session_manager.py
            â”‚    â””â”€ assistant/azor.py
            â”‚         â””â”€ assistant/assistent.py
            â”‚
            â””â”€ chat_session.py
                 â”œâ”€ assistant/assistent.py       (system prompt + nazwa)
                 â”œâ”€ files/session_files.py       (zapis/odczyt JSON)
                 â”œâ”€ files/wal.py                 (Write-Ahead Log)
                 â”‚    â””â”€ files/config.py
                 â”‚
                 â””â”€ llm/ (dynamiczny wybÃ³r na podstawie ENGINE)
                      â”œâ”€ gemini_client.py
                      â”‚    â””â”€ gemini_validation.py    (Pydantic)
                      â”œâ”€ llama_client.py
                      â”‚    â””â”€ llama_validation.py     (Pydantic)
                      â”œâ”€ ollama_client.py
                      â”‚    â””â”€ ollama_validation.py    (Pydantic)
                      â””â”€ anthropic_client.py
                           â””â”€ anthropic_validation.py (Pydantic)
```

### PrzepÅ‚yw danych â€” podsumowanie

```
UÅ¼ytkownik â†’ prompt_toolkit â†’ chat.main_loop()
                                    â”‚
                              SessionManager (singleton)
                                    â”‚
                              ChatSession
                               â”œâ”€ Assistant (toÅ¼samoÅ›Ä‡: "AZOR")
                               â”œâ”€ LLM Client (silnik) â† wybÃ³r: ENGINE env var
                               â”‚   â””â”€ Chat Session Wrapper
                               â”‚       â””â”€ SDK / Model API
                               â”œâ”€ WAL (bezpieczeÅ„stwo: append-only log)
                               â””â”€ Session Files (persystencja: JSON)
                                    â”‚
                              ~/.azor/{id}-log.json
                              ~/.azor/azor-wal.json
                              ~/.azor/output/{id}.pdf
```

---

*Dokument wygenerowany na podstawie analizy peÅ‚nego kodu ÅºrÃ³dÅ‚owego aplikacji Azor ChatDog (Python). Ostatnia aktualizacja: luty 2026.*



