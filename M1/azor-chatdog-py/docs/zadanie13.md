# Zadanie 13 ‚Äî Temperature, Top P, Top K w klientach LLM

## üéØ Cel zadania

Umo≈ºliwiƒá u≈ºytkownikowi konfiguracjƒô trzech kluczowych parametr√≥w **samplowania** (sampling) we wszystkich klientach LLM dzia≈ÇajƒÖcych w projekcie AZOR:

- **Temperature**
- **Top P** (nucleus sampling)
- **Top K**

Parametry te kontrolujƒÖ **jak model wybiera kolejny token** podczas generowania tekstu. SƒÖ fundamentalne dla dostrajania zachowania modelu ‚Äî od deterministycznego i precyzyjnego, po kreatywny i zaskakujƒÖcy.

---

## üìñ Czym sƒÖ te parametry? G≈Çƒôbokie wyja≈õnienie

### Jak model generuje tekst ‚Äî krok po kroku

Du≈ºy model jƒôzykowy (LLM) generuje tekst **token po tokenie**. Na ka≈ºdym kroku model oblicza **rozk≈Çad prawdopodobie≈Ñstwa** nad ca≈Çym s≈Çownikiem (vocabulary) ‚Äî np. 32 000 token√≥w. Ka≈ºdy token dostaje pewne prawdopodobie≈Ñstwo.

Przyk≈Çad ‚Äî model generuje nastƒôpne s≈Çowo po "Pies jest":

| Token        | Raw logit | Prawdopodobie≈Ñstwo (po softmax) |
|-------------|-----------|-------------------------------|
| wierny      | 5.2       | 0.35                          |
| du≈ºy        | 4.8       | 0.25                          |
| g≈Çodny      | 4.1       | 0.12                          |
| mƒÖdry       | 3.9       | 0.09                          |
| szybki      | 3.5       | 0.06                          |
| ... (32000) | ...       | ...                           |

**Bez ≈ºadnego samplowania** (greedy decoding) model zawsze wybierze "wierny" (najwy≈ºsze prawdopodobie≈Ñstwo). Jest to deterministyczne, ale nudne i powtarzalne.

Parametry samplowania modyfikujƒÖ ten rozk≈Çad **przed** wyborem tokena.

---

### üå°Ô∏è Temperature

**Co robi**: Skaluje logity (raw scores) przed zastosowaniem softmax. Dzieli ka≈ºdy logit przez warto≈õƒá temperature.

**Wz√≥r matematyczny**:

```
P(token_i) = exp(logit_i / T) / Œ£ exp(logit_j / T)
```

gdzie `T` = temperature.

**Efekt**:

| Temperature | Efekt na rozk≈Çad | Zachowanie modelu |
|------------|-----------------|-------------------|
| `0.0`      | Degeneruje do argmax (greedy) | Zawsze wybiera najwy≈ºej oceniony token. Deterministyczne, powtarzalne. |
| `0.1‚Äì0.3`  | Bardzo sp≈Çaszczone szczyty | Prawie deterministyczne, minimalna wariacja |
| `0.5‚Äì0.7`  | Umiarkowane wyg≈Çadzenie | Dobry balans: sp√≥jne, ale z lekkƒÖ kreatywno≈õciƒÖ |
| `1.0`      | Brak modyfikacji (orygina≈Ç) | Domy≈õlny rozk≈Çad prawdopodobie≈Ñstw modelu |
| `1.2‚Äì1.5`  | Wyr√≥wnanie rozk≈Çadu | Wiƒôcej "zaskakujƒÖcych" wybor√≥w, mniej przewidywalne |
| `2.0`      | Prawie r√≥wnomierny rozk≈Çad | Chaotyczne, czƒôsto niesp√≥jne odpowiedzi |

**Analogia**: Temperature to jak "rozgrzewanie" kostki do gry. Niska temperatura = kostka prawie zawsze lƒÖduje na jednej stronie. Wysoka temperatura = wszystkie strony r√≥wnie prawdopodobne.

**Kiedy u≈ºywaƒá**:
- **Niska (0.0‚Äì0.3)**: T≈Çumaczenia, odpowiedzi na pytania faktograficzne, kod
- **≈örednia (0.5‚Äì0.8)**: Dialog, asystent og√≥lnego przeznaczenia
- **Wysoka (1.0‚Äì1.5)**: Kreatywne pisanie, brainstorming, generowanie wielu wariant√≥w

---

### üéØ Top P (Nucleus Sampling)

**Co robi**: Zamiast rozwa≈ºaƒá wszystkie tokeny, Top P **obcina** rozk≈Çad do najmniejszego zestawu token√≥w, kt√≥rych skumulowane prawdopodobie≈Ñstwo przekracza warto≈õƒá `p`.

**Algorytm**:
1. Posortuj tokeny malejƒÖco wg prawdopodobie≈Ñstwa
2. Dodawaj tokeny po kolei, sumujƒÖc prawdopodobie≈Ñstwa
3. Zatrzymaj siƒô, gdy suma ‚â• `top_p`
4. Wylosuj token z tego podzbioru (z renormalizacjƒÖ)

**Przyk≈Çad** (top_p = 0.7):

| Token   | P     | Skumulowane P | W zestawie? |
|---------|-------|---------------|-------------|
| wierny  | 0.35  | 0.35          | ‚úÖ          |
| du≈ºy    | 0.25  | 0.60          | ‚úÖ          |
| g≈Çodny  | 0.12  | 0.72          | ‚úÖ (‚â• 0.7)  |
| mƒÖdry   | 0.09  | 0.81          | ‚ùå          |
| szybki  | 0.06  | 0.87          | ‚ùå          |

Model losuje tylko z {wierny, du≈ºy, g≈Çodny} ‚Äî 3 tokeny zamiast 32 000.

**Efekt warto≈õci**:

| Top P | Efekt |
|-------|-------|
| `0.1` | Bardzo wƒÖski zestaw (1-2 tokeny) ‚Äî prawie greedy |
| `0.5` | Umiarkowany ‚Äî kilka najlepszych token√≥w |
| `0.9` | Szeroki zestaw ‚Äî wiele opcji, ale wyklucza "ogon" rozk≈Çadu |
| `1.0` | Brak filtrowania ‚Äî wszystkie tokeny uwzglƒôdnione |

**Dlaczego Top P jest lepszy ni≈º sta≈Ça temperatura?** Poniewa≈º adaptuje siƒô do kontekstu. Gdy model jest "pewny" (jeden token dominuje), Top P wybiera ma≈Ço token√≥w. Gdy model jest "niepewny" (p≈Çaski rozk≈Çad), Top P dopuszcza wiƒôcej kandydat√≥w.

---

### üî¢ Top K

**Co robi**: Ogranicza wyb√≥r do `K` token√≥w o najwy≈ºszym prawdopodobie≈Ñstwie. Reszta jest odrzucana.

**Algorytm**:
1. Posortuj tokeny malejƒÖco wg prawdopodobie≈Ñstwa
2. We≈∫ pierwszych `K` token√≥w
3. Wylosuj token z tego podzbioru (z renormalizacjƒÖ)

**Przyk≈Çad** (top_k = 3):

Model rozwa≈ºa tylko: {wierny, du≈ºy, g≈Çodny} ‚Äî dok≈Çadnie 3 tokeny, niezale≈ºnie od ich prawdopodobie≈Ñstw.

**Efekt warto≈õci**:

| Top K | Efekt |
|-------|-------|
| `1`   | Greedy decoding ‚Äî zawsze najlepszy token |
| `10`  | Bardzo ograniczony wyb√≥r |
| `40`  | Typowa warto≈õƒá ‚Äî dobry balans |
| `100` | Szeroki wyb√≥r |
| `‚àû`   | Brak filtrowania (domy≈õlne zachowanie) |

**Wada Top K vs. Top P**: Top K jest "≈õlepy" na rozk≈Çad. Je≈õli jeden token ma P=0.95, a nastƒôpne 39 majƒÖ po P=0.001 ‚Äî Top K=40 nadal rozwa≈ºa te 39 bezu≈ºytecznych token√≥w. Top P=0.95 wybra≈Çby tylko 1 token. Dlatego Top P jest generalnie preferowany, ale Top K jest prostszy obliczeniowo.

---

### üîó Jak parametry wsp√≥≈Çdzia≈ÇajƒÖ?

Parametry sƒÖ stosowane **sekwencyjnie** (kolejno≈õƒá zale≈ºy od implementacji, ale typowo):

1. **Temperature** modyfikuje logity ‚Üí zmienia rozk≈Çad prawdopodobie≈Ñstwa
2. **Top K** obcina do K najlepszych token√≥w
3. **Top P** dalej zawƒô≈ºa do jƒÖdra (nucleus) prawdopodobie≈Ñstwa
4. Model losuje z wynikowego zestawu

**Rekomendowane kombinacje**:

| Scenariusz | Temperature | Top P | Top K |
|-----------|-------------|-------|-------|
| Precyzyjne odpowiedzi (FAQ, kod) | 0.0‚Äì0.2 | 1.0 | ‚Äî |
| Asystent og√≥lny (domy≈õlne Azor) | 0.7 | 0.9 | 40 |
| Kreatywne pisanie | 1.0‚Äì1.2 | 0.95 | 50‚Äì100 |
| Brainstorming (max kreatywno≈õƒá) | 1.5 | 1.0 | 100 |

**Uwaga**: Najlepiej modyfikowaƒá **jeden** parametr naraz i obserwowaƒá efekt. Zmienianie wszystkich trzech jednocze≈õnie utrudnia zrozumienie wp≈Çywu ka≈ºdego z nich.

---

## üîß R√≥≈ºnice konfiguracji miƒôdzy klientami LLM

### Tabela por√≥wnawcza

| Parametr | Gemini (google-genai) | LLaMA (llama-cpp-python) | Ollama | Anthropic |
|----------|----------------------|-------------------------|--------|-----------|
| **Temperature** | ‚úÖ `temperature` w `GenerateContentConfig` | ‚úÖ `temperature` w `Llama.__call__()` | ‚úÖ `temperature` w `options` dict | ‚úÖ `temperature` w `messages.create()` |
| **Top P** | ‚úÖ `top_p` w `GenerateContentConfig` | ‚úÖ `top_p` w `Llama.__call__()` | ‚úÖ `top_p` w `options` dict | ‚úÖ `top_p` w `messages.create()` |
| **Top K** | ‚úÖ `top_k` w `GenerateContentConfig` | ‚úÖ `top_k` w `Llama.__call__()` | ‚úÖ `top_k` w `options` dict | ‚úÖ `top_k` w `messages.create()` |
| **Zakres Temperature** | 0.0 ‚Äì 2.0 | 0.0 ‚Äì ‚àû (praktycznie 0.0‚Äì2.0) | 0.0 ‚Äì 2.0 | 0.0 ‚Äì 1.0 |
| **Zakres Top P** | 0.0 ‚Äì 1.0 | 0.0 ‚Äì 1.0 | 0.0 ‚Äì 1.0 | 0.0 ‚Äì 1.0 |
| **Zakres Top K** | ‚â• 1 (int) | ‚â• 1 (int) | ‚â• 1 (int) | ‚â• 1 (int) |
| **Domy≈õlna Temperature** | Zale≈ºy od modelu (~1.0) | 0.8 | 0.8 | 1.0 |
| **Domy≈õlne Top P** | Zale≈ºy od modelu (~0.95) | 0.95 | 0.9 | 0.999 |
| **Domy≈õlne Top K** | Zale≈ºy od modelu (~40) | 40 | 40 | Brak domy≈õlnego (wy≈ÇƒÖczone, gdy nie ustawione) |

---

### Gemini (google-genai)

**Dokumentacja**: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters

Parametry przekazywane w obiekcie `GenerateContentConfig`:

```python
from google.genai import types

config = types.GenerateContentConfig(
    system_instruction=system_instruction,
    temperature=0.7,        # float, 0.0‚Äì2.0
    top_p=0.9,              # float, 0.0‚Äì1.0
    top_k=40,               # int, >= 1
    thinking_config=types.ThinkingConfig(thinking_budget=0)
)

session = client.chats.create(model=model_name, config=config)
```

**Uwagi Gemini**:
- Parametry ustawia siƒô na poziomie konfiguracji sesji czatu
- `GenerateContentConfig` przyjmuje wszystkie trzy parametry bezpo≈õrednio
- Gdy ustawiasz `thinking_budget > 0` (tryb my≈õlenia), temperature jest ignorowane

---

### LLaMA (llama-cpp-python)

**Dokumentacja**: https://llama-cpp-python.readthedocs.io/en/latest/api-reference/

Parametry przekazywane bezpo≈õrednio w wywo≈Çaniu `Llama.__call__()`:

```python
output = llama_model(
    prompt,
    max_tokens=512,
    temperature=0.7,    # float
    top_p=0.9,          # float, 0.0‚Äì1.0
    top_k=40,           # int
    stop=["User:", "Assistant:"],
    echo=False,
)
```

**Uwagi LLaMA**:
- Parametry ustawia siƒô **per wywo≈Çanie** (nie per sesja)
- `temperature=0.0` oznacza greedy decoding
- Dodatkowe parametry niedostƒôpne w innych klientach: `repeat_penalty`, `frequency_penalty`, `presence_penalty`, `mirostat_mode`, `mirostat_tau`, `mirostat_eta`
- llama-cpp-python nie nak≈Çada sztywnego g√≥rnego limitu na temperature, ale warto≈õci > 2.0 dajƒÖ chaotyczne wyniki

---

### Ollama

**Dokumentacja**: https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values

Parametry przekazywane w dict `options` w wywo≈Çaniu `chat()`:

```python
response = ollama_client.chat(
    model=model_name,
    messages=messages,
    options={
        "temperature": 0.7,   # float, 0.0‚Äì2.0
        "top_p": 0.9,         # float, 0.0‚Äì1.0
        "top_k": 40,          # int, >= 1
    },
    stream=False,
)
```

**Uwagi Ollama**:
- Parametry przekazywane jako dict `options` ‚Äî nie jako named kwargs
- Ollama przekazuje te parametry do underlying modelu (llama.cpp, itp.)
- Wspiera dodatkowe parametry: `num_predict`, `repeat_penalty`, `seed`, `num_ctx`, `mirostat`
- Serwer Ollama mo≈ºe nadpisaƒá parametry zdefiniowane w `Modelfile` modelu

---

### Anthropic

**Dokumentacja**: https://docs.anthropic.com/en/api/messages

Parametry przekazywane bezpo≈õrednio w `messages.create()`:

```python
response = anthropic_client.messages.create(
    model=model_name,
    max_tokens=4096,
    system=system_instruction,
    messages=messages,
    temperature=0.7,    # float, 0.0‚Äì1.0
    top_p=0.9,          # float, 0.0‚Äì1.0
    top_k=40,           # int, ‚â• 1
)
```

**Uwagi Anthropic**:
- Zakres `temperature` jest ograniczony do **0.0‚Äì1.0** (mniejszy ni≈º u Gemini/Ollama)
- `temperature=0.0` jest w pe≈Çni deterministyczne
- `top_k` jest wspierane ‚Äî gdy nie ustawione, Anthropic nie stosuje filtrowania Top-K (efektywnie top_k=‚àû)
- Anthropic zaleca: "Je≈õli u≈ºywasz `top_p`, ustaw `temperature` na 1.0" ‚Äî i odwrotnie

---

## üîê Zmienne ≈õrodowiskowe

Ka≈ºdy klient ma sw√≥j prefiks, a parametry samplowania majƒÖ suffix `_TEMPERATURE`, `_TOP_P`, `_TOP_K`:

| Zmienna | Typ | Domy≈õlnie | Klient |
|---------|-----|-----------|--------|
| `GEMINI_TEMPERATURE` | float | `None` (domy≈õlne API) | Gemini |
| `GEMINI_TOP_P` | float | `None` | Gemini |
| `GEMINI_TOP_K` | int | `None` | Gemini |
| `LLAMA_TEMPERATURE` | float | `None` | LLaMA |
| `LLAMA_TOP_P` | float | `None` | LLaMA |
| `LLAMA_TOP_K` | int | `None` | LLaMA |
| `OLLAMA_TEMPERATURE` | float | `None` | Ollama |
| `OLLAMA_TOP_P` | float | `None` | Ollama |
| `OLLAMA_TOP_K` | int | `None` | Ollama |
| `ANTHROPIC_TEMPERATURE` | float | `None` (domy≈õlne API) | Anthropic |
| `ANTHROPIC_TOP_P` | float | `None` | Anthropic |
| `ANTHROPIC_TOP_K` | int | `None` | Anthropic |

**Warto≈õƒá `None`** oznacza: "u≈ºyj domy≈õlnej warto≈õci biblioteki/API". Dziƒôki temu bez ustawiania zmiennych nic siƒô nie zmienia ‚Äî zachowanie jest identyczne jak przed zadaniem 13.

### Przyk≈Çad `.env`

```env
# Gemini z niskƒÖ temperaturƒÖ i nucleus sampling
ENGINE=GEMINI
GEMINI_API_KEY=your-key-here
MODEL_NAME=gemini-2.5-flash
GEMINI_TEMPERATURE=0.3
GEMINI_TOP_P=0.85

# LLaMA z wy≈ºszƒÖ kreatywno≈õciƒÖ
ENGINE=LLAMA_CPP
LLAMA_MODEL_PATH=/path/to/model.gguf
LLAMA_TEMPERATURE=1.0
LLAMA_TOP_P=0.95
LLAMA_TOP_K=50

# Ollama
ENGINE=OLLAMA
OLLAMA_MODEL_NAME=qwen2.5:7b-instruct
OLLAMA_TEMPERATURE=0.7
OLLAMA_TOP_K=40

# Anthropic
ENGINE=ANTHROPIC
ANTHROPIC_API_KEY=your-key-here
ANTHROPIC_TEMPERATURE=0.5
ANTHROPIC_TOP_P=0.9
ANTHROPIC_TOP_K=40
```

---

## üìê Przebieg implementacji

### Krok 1: Rozszerzenie walidacji Pydantic

W ka≈ºdym pliku `*_validation.py` dodano pola `temperature`, `top_p`, `top_k` (jako `Optional`) z odpowiednimi zakresami warto≈õci i walidacjƒÖ. Warto≈õƒá `None` = domy≈õlna biblioteki.

### Krok 2: Rozszerzenie konstruktor√≥w klient√≥w

W `__init__()` ka≈ºdego klienta dodano parametry `temperature`, `top_p`, `top_k`. W `from_environment()` odczytuje siƒô je z odpowiednich zmiennych ≈õrodowiskowych.

### Krok 3: Propagacja do sesji czatu

Parametry z klienta przekazywane sƒÖ do obiekt√≥w sesji (`LlamaChatSession`, `OllamaChatSession`, `AnthropicChatSession`) ‚Äî a w przypadku Gemini wstawiane do `GenerateContentConfig`.

### Krok 4: U≈ºycie w wywo≈Çaniach API

Ka≈ºda sesja u≈ºywa parametr√≥w samplowania w swoich wywo≈Çaniach API:
- **LLaMA**: `self.llama_model(prompt, temperature=..., top_p=..., top_k=...)`
- **Ollama**: `self.ollama_client.chat(..., options={...})`
- **Gemini**: `types.GenerateContentConfig(temperature=..., top_p=..., top_k=...)`
- **Anthropic**: `self.anthropic_client.messages.create(temperature=..., top_p=..., top_k=...)`

### Krok 5: Informacja w wiadomo≈õci powitalnej

Metoda `ready_for_use_message()` wy≈õwietla ustawione parametry samplowania, np.:
```
‚úÖ Klient Ollama gotowy do u≈ºycia (Model: qwen2.5:7b-instruct, Host: ..., T=0.7, TopP=0.9, TopK=40)
```

---

## üß† Kluczowe wnioski

1. **Temperature, Top P i Top K to filtry na rozk≈Çad prawdopodobie≈Ñstwa** ‚Äî ka≈ºdy dzia≈Ça inaczej, ale wszystkie ograniczajƒÖ "losowo≈õƒá" modelu
2. **Top P jest adaptacyjny** (automatycznie dostosowuje liczbƒô kandydat√≥w), Top K jest sta≈Çy
3. **Wszystkie cztery klienty wspierajƒÖ temperature, top_p i top_k** ‚Äî pe≈Çna parno≈õƒá funkcji
4. **Anthropic ma wƒô≈ºszy zakres temperature (0.0‚Äì1.0)** ‚Äî inne klienty pozwalajƒÖ na 0.0‚Äì2.0
5. **Domy≈õlne `None`** gwarantuje wstecznƒÖ kompatybilno≈õƒá ‚Äî bez ustawiania zmiennych zachowanie siƒô nie zmienia
6. **Najlepsza praktyka**: zmieniaj jeden parametr naraz, obserwuj efekt, potem dostrajaj kolejny

